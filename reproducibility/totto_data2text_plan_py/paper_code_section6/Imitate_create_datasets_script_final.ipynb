{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TRAIN_DATASET.pickle\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_VALID_DATASET.pickle\", \"rb\") as f:\n",
    "    valid_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TEST_DATASET.pickle\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce same output as the script *create_dataset* from data2text-plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "section_content -------- true text   \n",
    "\n",
    "entity_recognition  ------- actually input - set of records   \n",
    "\n",
    "Set of records in order they appear in section_content    -------- content plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_DELIM = \" \"\n",
    "DELIM = u\"ï¿¨\"\n",
    "\n",
    "HOME = \"HOME\"\n",
    "AWAY = \"AWAY\"\n",
    "\n",
    "# ENTITY = \"Indication\"\n",
    "ENTITY = \"Section6\"\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure - sorted entities\n",
    "def _sort_key(entity):\n",
    "    return entity['BeginOffset']\n",
    "\n",
    "def test_order_entities(section_entities):\n",
    "    \n",
    "    sorted_entities = sorted(section_entities, key=_sort_key)\n",
    "    \n",
    "    if sorted_entities == section_entities: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def test_order_dataset(dataset):\n",
    "    for leaflet in dataset:\n",
    "\n",
    "        current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                    leaflet.section3, leaflet.section4, \n",
    "                                    leaflet.section5, leaflet.section6]\n",
    "\n",
    "        for current_section in current_leaflet_sections:\n",
    "\n",
    "            if current_section.entity_recognition is None:\n",
    "                continue\n",
    "\n",
    "            assert test_order_entities(current_section.entity_recognition) == True\n",
    "\n",
    "            \n",
    "test_order_dataset(train_dataset)\n",
    "test_order_dataset(valid_dataset)\n",
    "test_order_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_contentplan(dataset):\n",
    "    \"\"\"\n",
    "    Transform dataset to be a suitable format for model data2text-plan\n",
    "    \"\"\"\n",
    "    \n",
    "    # array to store section1 of each leaflet\n",
    "    summaries_leaflets = []\n",
    "    \n",
    "    # array to store content plan of each leaflet\n",
    "    content_plan_leaflets = []\n",
    "    \n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        # extract section1 content\n",
    "        section1_content = leaflet.section6.section_content\n",
    "        # extract results of NER\n",
    "        section1_entity_recognition = leaflet.section6.entity_recognition\n",
    "        \n",
    "        # skip if either input or output is None\n",
    "        if section1_content is None or section1_entity_recognition is None:\n",
    "            continue\n",
    "        \n",
    "        # get the content plan of each section\n",
    "        content_plan_section1 = ''\n",
    "\n",
    "        for entity in section1_entity_recognition:\n",
    "            entity_value = entity['Text'] if len(entity['Text'].split(\" \")) == 0 else (\"_\").join(entity['Text'].split(\" \"))\n",
    "            entity_type = entity['Type'] if entity['Type'] is not None and len(entity['Type']) > 0 else entity['Category']\n",
    "            \n",
    "            # randomly choose HOME or AWAY\n",
    "            if random.randint(1,2) == 1:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + HOME\n",
    "            else:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + AWAY\n",
    "            \n",
    "            if section1_entity_recognition.index(entity) != len(section1_entity_recognition) - 1:\n",
    "                content_plan_section1 += \" \"\n",
    "            else:\n",
    "                content_plan_section1 += \" \" + \"\\n\"\n",
    "\n",
    "\n",
    "        content_plan_leaflets.append(content_plan_section1)\n",
    "\n",
    "        # get the section1 content\n",
    "        # make sure to have punctuations as a separate token\n",
    "        section1_content = wordpunct_tokenize(section1_content)\n",
    "        \n",
    "        # back to string\n",
    "        section1_content = \" \".join(section1_content)\n",
    "        \n",
    "        # add \"\\n\" at the end\n",
    "        section1_content = section1_content + \"\\n\"\n",
    "        \n",
    "        summaries_leaflets.append(section1_content)\n",
    "    \n",
    "    return (content_plan_leaflets, summaries_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_records(records):\n",
    "    \"\"\"\n",
    "    To src_train.txt and src_valid.txt pre-append these special characters, according to data2text-plan project\n",
    "    \"\"\"\n",
    "    \n",
    "    record = []\n",
    "    record.append(UNK_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(BOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(EOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the src_train - training data to be input to the model\n",
    "\n",
    "def create_src_table(content_plan_leaflets):\n",
    "    \"\"\"\n",
    "    Create src_train - input \"table\" to the model\n",
    "    \n",
    "    Idea: - we do not have a table, so randomized the records in content plan\n",
    "    \n",
    "    Update: - do not randomize - make it easier for the model to learn\n",
    "    \"\"\"\n",
    "    \n",
    "    # store input \"table\" of each leaflet in array\n",
    "    src_leaflets = []\n",
    "    \n",
    "    for leaflet_content_plan in content_plan_leaflets:\n",
    "        # remove the end symbol ('\\n') of the string\n",
    "        leaflet_content_plan = leaflet_content_plan[:-2]\n",
    "\n",
    "        # split string into a list of records\n",
    "        leaflet_content_plan_collection = leaflet_content_plan.split(\" \")\n",
    "        \n",
    "        # do not do\n",
    "        # randomly shuffle records in a list\n",
    "        # random.shuffle(leaflet_content_plan_collection)\n",
    "        \n",
    "        # add special symbols to the begining\n",
    "        special_symbols = []\n",
    "        special_symbols = add_special_records(special_symbols)\n",
    "        \n",
    "        # create a string containing all the records and special_symbols in the begining\n",
    "        src_leaflet_section1 = ''\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(special_symbols)\n",
    "        src_leaflet_section1 += \" \"\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(leaflet_content_plan_collection)\n",
    "        src_leaflet_section1 += '\\n'\n",
    "\n",
    "\n",
    "        src_leaflets.append(src_leaflet_section1)\n",
    "    \n",
    "    return src_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE = 'src_train.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT = 'train_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_train, leaflets_inter_contentplan_train, leaflets_tgt_train = create_training_sets(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    src_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tgt_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    inter_train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what cystagon contains - the active substance is cysteamine bitartrate ( mercaptamine bitartrate ). each hard capsule of cystagon 50 mg contains 50 mg of cysteamine ( as mercaptamine bitartrate ) each hard capsule of cystagon 150 mg contains 150 mg of cysteamine ( as mercaptamine bitartrate ) - the other ingredients are microcrystalline cellulose , starch , pregelatinized , magnesium stearate / sodium lauryl sulphate , colloidal silicon dioxide , croscarmellose sodium , capsule shells : gelatin , titanium dioxide , black ink on hard capsules ( e172 ). what cystagon looks like and contents of the pack hard capsules - cystagon 50 mg : white , opaque hard capsules with cysta 50 on the body and mylan on the cap . bottles of 100 or 500 hard capsules . all pack sizes may be not marketed . - cystagon 150 mg : white , opaque hard capsules with cystagon 150 on the body and mylan on the cap . bottles of 100 or 500 hard capsules . all pack sizes may be not marketed .\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "## Validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_sets(dataset):\n",
    "    \n",
    "    # Output files    \n",
    "    INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN_VALID, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_VALID, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_VALID, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT_VALID, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_valid, leaflets_inter_contentplan_valid, leaflets_tgt_valid = create_validation_sets(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/valid_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    contentplan_valid = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================\n",
    "## Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    SRC_FILE_TEST = 'test/src_test.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_TEST = \"test/tgt_test.txt\"  # tgt file of second stage \n",
    "    \n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to just summary and src data\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_TEST, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_TEST, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "\n",
    "    return src_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_test, leaflets_tgt_test = create_test_sets(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================\n",
    "## Creating *train-roto-ptrs.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "OUTPUT_FILE = \"train-roto-ptrs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_TGT_FILE) as reader:\n",
    "    leaflet_tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INTER_CONTENT_PLAN) as reader:\n",
    "    leaflets_inter_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg: the last entry 245,39 in train_roto_ptrs[1] indicates that the 245th token in summary matches with 39th content plan entry.  \n",
    "\n",
    "Phoenix ----> Phoenixï¿¨Sunsï¿¨TEAM-CITYï¿¨HOME  \n",
    "Suns ----> Sunsï¿¨Sunsï¿¨TEAM-NAMEï¿¨HOME  \n",
    "39 ----> 39ï¿¨Sunsï¿¨TEAM-WINSï¿¨HOME  \n",
    "38 ----> 38ï¿¨Sunsï¿¨TEAM-LOSSESï¿¨HOME  \n",
    "87 ----> 87ï¿¨Sunsï¿¨TEAM-PTSï¿¨HOME  \n",
    "85 ----> 85ï¿¨Jazzï¿¨TEAM-PTSï¿¨AWAY  \n",
    "Utah ----> Utahï¿¨Jazzï¿¨TEAM-CITYï¿¨AWAY  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what cystagon contains - the active substance is cysteamine bitartrate ( mercaptamine bitartrate ). each hard capsule of cystagon 50 mg contains 50 mg of cysteamine ( as mercaptamine bitartrate ) each hard capsule of cystagon 150 mg contains 150 mg of cysteamine ( as mercaptamine bitartrate ) - the other ingredients are microcrystalline cellulose , starch , pregelatinized , magnesium stearate / sodium lauryl sulphate , colloidal silicon dioxide , croscarmellose sodium , capsule shells : gelatin , titanium dioxide , black ink on hard capsules ( e172 ). what cystagon looks like and contents of the pack hard capsules - cystagon 50 mg : white , opaque hard capsules with cysta 50 on the body and mylan on the cap . bottles of 100 or 500 hard capsules . all pack sizes may be not marketed . - cystagon 150 mg : white , opaque hard capsules with cystagon 150 on the body and mylan on the cap . bottles of 100 or 500 hard capsules . all pack sizes may be not marketed .\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cystagonï¿¨Section6ï¿¨PRODUCT_NAMEï¿¨HOME cysteamine_bitartrate_(mercaptamine_bitartrateï¿¨Section6ï¿¨TREATMENTï¿¨HOME cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY 50ï¿¨Section6ï¿¨NUMBERï¿¨HOME 50ï¿¨Section6ï¿¨NUMBERï¿¨AWAY cysteamine_(as_mercaptamine_bitartrate)ï¿¨Section6ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME 150ï¿¨Section6ï¿¨NUMBERï¿¨AWAY 150ï¿¨Section6ï¿¨NUMBERï¿¨AWAY cysteamine_(as_mercaptamine_bitartrate)ï¿¨Section6ï¿¨TREATMENTï¿¨AWAY microcrystalline_celluloseï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME starchï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME pregelatinizedï¿¨Section6ï¿¨TREATMENTï¿¨AWAY magnesium_stearateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY sodium_lauryl_sulphateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME colloidal_silicon_dioxideï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY croscarmellose_sodiumï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME capsule_shellsï¿¨Section6ï¿¨TREATMENTï¿¨AWAY gelatin,_titanium_dioxideï¿¨Section6ï¿¨TREATMENTï¿¨HOME black_inkï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME the_pack_hard_capsulesï¿¨Section6ï¿¨TREATMENTï¿¨HOME cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME 50ï¿¨Section6ï¿¨NUMBERï¿¨HOME white,_opaque_hard_capsulesï¿¨Section6ï¿¨TREATMENTï¿¨AWAY cystaï¿¨Section6ï¿¨TREATMENTï¿¨AWAY 50ï¿¨Section6ï¿¨NUMBERï¿¨HOME mylanï¿¨Section6ï¿¨BRAND_NAMEï¿¨HOME 100ï¿¨Section6ï¿¨NUMBERï¿¨AWAY 500ï¿¨Section6ï¿¨NUMBERï¿¨HOME all_pack_sizesï¿¨Section6ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME 150ï¿¨Section6ï¿¨NUMBERï¿¨AWAY cystagonï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME 150ï¿¨Section6ï¿¨NUMBERï¿¨HOME mylanï¿¨Section6ï¿¨BRAND_NAMEï¿¨HOME 100ï¿¨Section6ï¿¨NUMBERï¿¨AWAY 500ï¿¨Section6ï¿¨NUMBERï¿¨HOME all_pack_sizesï¿¨Section6ï¿¨TREATMENTï¿¨HOME \\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_pts_content = []\n",
    "\n",
    "# for each leaflet\n",
    "for leflet_num in range(len(leaflet_tgt_train)):\n",
    "    \n",
    "    # get current leaflet and content plan\n",
    "    current_leaflet = leaflet_tgt_train[leflet_num].split()\n",
    "    current_content_plan = leaflets_inter_content_plan[leflet_num].split()\n",
    "    \n",
    "    # get the values of content plan\n",
    "    instances = []\n",
    "    for entry in current_content_plan:\n",
    "        record_values = entry.split(DELIM)[0]\n",
    "        instances.append(record_values)\n",
    "    \n",
    "    # pairs (index_tgt, index_contentplan) for each leaflet\n",
    "    current_str = []\n",
    "    \n",
    "    # for each token in current summary\n",
    "    for token_pos in range(len(current_leaflet)):\n",
    "        \n",
    "        # get token\n",
    "        token = current_leaflet[token_pos]\n",
    "        \n",
    "        # possible tokens if 2 words in content plan like 'immunodeficiency_syndrome'\n",
    "        if token_pos < (len(current_leaflet)-1):\n",
    "            token_2words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1]\n",
    "        else:\n",
    "            token_2words = 'something that would never be in the section content'\n",
    "        \n",
    "        ### my-new-change\n",
    "        # possible tokens if 3 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-2):\n",
    "            token_3words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2]\n",
    "        else:\n",
    "            token_3words = 'something that would never be in the section content'\n",
    "        \n",
    "        # possible tokens if 4 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-3):\n",
    "            token_4words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2] + \"_\" + current_leaflet[token_pos+3]\n",
    "        else:\n",
    "            token_4words = 'something that would never be in the section content'\n",
    "        \n",
    "        \n",
    "        for content_plan_index in range(len(instances)):\n",
    "                \n",
    "            if token_4words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_3words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_2words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "    \n",
    "    # join pairs into string with \" \" between pairs\n",
    "    current_str = \" \".join(current_str)\n",
    "    \n",
    "    # add \"\\n\" at the end\n",
    "    current_str += \"\\n\"\n",
    "    \n",
    "    roto_pts_content.append(current_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"train-roto-ptrs.txt\"\n",
    "\n",
    "src_file = open(OUTPUT_FILE, 'w')\n",
    "for src_instance in roto_pts_content:\n",
    "    src_file.write(src_instance)\n",
    "src_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1042"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,0 8,2 18,4 20,5 22,6 33,8 40,9 43,10 46,11 49,12 52,13 56,14 59,15 64,23 70,17 73,18 76,19 79,20 85,21 87,1 92,24 95,25 98,26 101,27 105,22 115,28 123,31 141,33 143,34 145,35\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roto_pts_content[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content[index].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaflets_inter_content_plan[index].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zyclara', 'contains', '-', 'the'] ----> zyclaraï¿¨Section6ï¿¨PRODUCT_NAMEï¿¨AWAY\n",
      "['imiquimod', '.', 'each', 'sachet'] ----> imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['imiquimod', 'in', '250', 'mg'] ----> imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['250', 'mg', 'cream', '('] ----> 250ï¿¨Section6ï¿¨NUMBERï¿¨AWAY\n",
      "['cream', '(', '100', 'mg'] ----> creamï¿¨Section6ï¿¨TREATMENTï¿¨AWAY\n",
      "['imiquimod', ').', '-', 'the'] ----> imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['isostearic', 'acid', ',', 'benzyl'] ----> isostearic_acidï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['benzyl', 'alcohol', ',', 'cetyl'] ----> benzyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['cetyl', 'alcohol', ',', 'stearyl'] ----> cetyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['stearyl', 'alcohol', ',', 'white'] ----> stearyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['white', 'soft', 'paraffin', ','] ----> white_soft_paraffinï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['polysorbate', '60', ',', 'sorbitan'] ----> polysorbate_60ï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['sorbitan', 'stearate', ',', 'glycerol'] ----> sorbitan_stearateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['methyl', 'parahydroxybenzoate', '(', 'e'] ----> methyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['propyl', 'parahydroxybenzoate', '(', 'e'] ----> propyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['e', '216', '),', 'xanthan'] ----> e_216ï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['xanthan', 'gum', ',', 'purified'] ----> xanthan_gumï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['purified', 'water', '(', 'see'] ----> purified_waterï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['2', '\"', 'zyclara', 'contains'] ----> 2ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "['zyclara', 'contains', 'methyl', 'parahydroxybenzoate'] ----> zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨AWAY\n",
      "['propyl', 'parahydroxybenzoate', ',', 'cetyl'] ----> propyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['cetyl', 'alcohol', ',', 'stearyl'] ----> cetyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['stearyl', 'alcohol', 'and', 'benzyl'] ----> stearyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['benzyl', 'alcohol', '\").', 'what'] ----> benzyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['zyclara', 'looks', 'like', 'and'] ----> zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨HOME\n",
      "['zyclara', '3', '.', '75'] ----> zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨AWAY\n",
      "['250', 'mg', 'of', 'a'] ----> 250ï¿¨Section6ï¿¨NUMBERï¿¨AWAY\n",
      "['14', ',', '28', 'or'] ----> 14ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "['28', 'or', '56', 'single'] ----> 28ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "['56', 'single', '-', 'use'] ----> 56ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n"
     ]
    }
   ],
   "source": [
    "content_plan_indeces = []\n",
    "\n",
    "for pair in roto_pts_content[index].split():\n",
    "    pair = pair.split(\",\")\n",
    "    \n",
    "    a = int(pair[0])\n",
    "    b = int(pair[1])\n",
    "    \n",
    "    content_plan_indeces.append(b)\n",
    "    \n",
    "    print(leaflet_tgt_train[index].split()[a:a+4], end=\" ----> \")\n",
    "    print(leaflets_inter_content_plan[index].split()[b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.375ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "3.75ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "glycerol,_methyl_parahydroxybenzoate_(eï¿¨Section6ï¿¨TREATMENTï¿¨HOME\n",
      "the_pack_-_each_zyclaraï¿¨Section6ï¿¨TREATMENTï¿¨AWAY\n",
      "3.75ï¿¨Section6ï¿¨NUMBERï¿¨HOME\n",
      "a_white_to_slightly_yellow_creamï¿¨Section6ï¿¨TREATMENTï¿¨AWAY\n",
      "polyester/_white_low_density_polyethylene/aluminium_foil_sachetsï¿¨Section6ï¿¨TREATMENTï¿¨AWAY\n"
     ]
    }
   ],
   "source": [
    "# check out pairs missed\n",
    "for i in range(0, len(leaflets_inter_content_plan[index].split()), 1):\n",
    "    if i not in content_plan_indeces:\n",
    "        print(leaflets_inter_content_plan[index].split()[i])\n",
    "        \n",
    "# explanation - 3-word long token\n",
    "# explanation - hiv_infection ---> bc NER outputs - 'hiv', 'hiv_infection' - in content plan I have 2 tokens starting with 'hiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what zyclara contains - the active substance is imiquimod . each sachet contains 9 . 375 mg of imiquimod in 250 mg cream ( 100 mg of cream contains 3 . 75 mg imiquimod ). - the other ingredients are isostearic acid , benzyl alcohol , cetyl alcohol , stearyl alcohol , white soft paraffin , polysorbate 60 , sorbitan stearate , glycerol , methyl parahydroxybenzoate ( e 218 ), propyl parahydroxybenzoate ( e 216 ), xanthan gum , purified water ( see also section 2 \" zyclara contains methyl parahydroxybenzoate , propyl parahydroxybenzoate , cetyl alcohol , stearyl alcohol and benzyl alcohol \"). what zyclara looks like and contents of the pack - each zyclara 3 . 75 % cream sachet contains 250 mg of a white to slightly yellow cream with a uniform appearance . - each box contains 14 , 28 or 56 single - use polyester / white low density polyethylene / aluminium foil sachets . not all pack sizes may be marketed .\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zyclaraï¿¨Section6ï¿¨PRODUCT_NAMEï¿¨AWAY zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨AWAY imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME 9.375ï¿¨Section6ï¿¨NUMBERï¿¨HOME imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY 250ï¿¨Section6ï¿¨NUMBERï¿¨AWAY creamï¿¨Section6ï¿¨TREATMENTï¿¨AWAY 3.75ï¿¨Section6ï¿¨NUMBERï¿¨HOME imiquimodï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY isostearic_acidï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY benzyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME cetyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME stearyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME white_soft_paraffinï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY polysorbate_60ï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY sorbitan_stearateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME glycerol,_methyl_parahydroxybenzoate_(eï¿¨Section6ï¿¨TREATMENTï¿¨HOME propyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY e_216ï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME xanthan_gumï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY purified_waterï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY 2ï¿¨Section6ï¿¨NUMBERï¿¨HOME zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨HOME methyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME propyl_parahydroxybenzoateï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY cetyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME stearyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨HOME benzyl_alcoholï¿¨Section6ï¿¨GENERIC_NAMEï¿¨AWAY zyclaraï¿¨Section6ï¿¨BRAND_NAMEï¿¨AWAY the_pack_-_each_zyclaraï¿¨Section6ï¿¨TREATMENTï¿¨AWAY 3.75ï¿¨Section6ï¿¨NUMBERï¿¨HOME 250ï¿¨Section6ï¿¨NUMBERï¿¨AWAY a_white_to_slightly_yellow_creamï¿¨Section6ï¿¨TREATMENTï¿¨AWAY 14ï¿¨Section6ï¿¨NUMBERï¿¨HOME 28ï¿¨Section6ï¿¨NUMBERï¿¨HOME 56ï¿¨Section6ï¿¨NUMBERï¿¨HOME polyester/_white_low_density_polyethylene/aluminium_foil_sachetsï¿¨Section6ï¿¨TREATMENTï¿¨AWAY \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
