{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TRAIN_DATASET.pickle\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_VALID_DATASET.pickle\", \"rb\") as f:\n",
    "    valid_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TEST_DATASET.pickle\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce same output as the script *create_dataset* from data2text-plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "section_content -------- true text   \n",
    "\n",
    "entity_recognition  ------- actually input - set of records   \n",
    "\n",
    "Set of records in order they appear in section_content    -------- content plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_DELIM = \" \"\n",
    "DELIM = u\"￨\"\n",
    "\n",
    "HOME = \"HOME\"\n",
    "AWAY = \"AWAY\"\n",
    "\n",
    "# ENTITY = \"Indication\"\n",
    "ENTITY = \"Section1\"\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure - sorted entities\n",
    "def _sort_key(entity):\n",
    "    return entity['BeginOffset']\n",
    "\n",
    "def test_order_entities(section_entities):\n",
    "    \n",
    "    sorted_entities = sorted(section_entities, key=_sort_key)\n",
    "    \n",
    "    if sorted_entities == section_entities: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def test_order_dataset(dataset):\n",
    "    for leaflet in dataset:\n",
    "\n",
    "        current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                    leaflet.section3, leaflet.section4, \n",
    "                                    leaflet.section5, leaflet.section6]\n",
    "\n",
    "        for current_section in current_leaflet_sections:\n",
    "\n",
    "            if current_section.entity_recognition is None:\n",
    "                continue\n",
    "\n",
    "            assert test_order_entities(current_section.entity_recognition) == True\n",
    "\n",
    "            \n",
    "test_order_dataset(train_dataset)\n",
    "test_order_dataset(valid_dataset)\n",
    "test_order_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_contentplan(dataset):\n",
    "    \"\"\"\n",
    "    Transform dataset to be a suitable format for model data2text-plan\n",
    "    \"\"\"\n",
    "    \n",
    "    # array to store section1 of each leaflet\n",
    "    summaries_leaflets = []\n",
    "    \n",
    "    # array to store content plan of each leaflet\n",
    "    content_plan_leaflets = []\n",
    "    \n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        # extract section1 content\n",
    "        section1_content = leaflet.section1.section_content\n",
    "        # extract results of NER\n",
    "        section1_entity_recognition = leaflet.section1.entity_recognition\n",
    "        \n",
    "        # skip if either input or output is None\n",
    "        if section1_content is None or section1_entity_recognition is None:\n",
    "            continue\n",
    "        \n",
    "        # get the content plan of each section\n",
    "        content_plan_section1 = ''\n",
    "\n",
    "        for entity in section1_entity_recognition:\n",
    "            entity_value = entity['Text'] if len(entity['Text'].split(\" \")) == 0 else (\"_\").join(entity['Text'].split(\" \"))\n",
    "            entity_type = entity['Type'] if entity['Type'] is not None and len(entity['Type']) > 0 else entity['Category']\n",
    "            \n",
    "            # randomly choose HOME or AWAY\n",
    "            if random.randint(1,2) == 1:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + HOME\n",
    "            else:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + AWAY\n",
    "            \n",
    "            if section1_entity_recognition.index(entity) != len(section1_entity_recognition) - 1:\n",
    "                content_plan_section1 += \" \"\n",
    "            else:\n",
    "                content_plan_section1 += \" \" + \"\\n\"\n",
    "\n",
    "\n",
    "        content_plan_leaflets.append(content_plan_section1)\n",
    "\n",
    "        # get the section1 content\n",
    "        # make sure to have punctuations as a separate token\n",
    "        section1_content = wordpunct_tokenize(section1_content)\n",
    "        \n",
    "        # back to string\n",
    "        section1_content = \" \".join(section1_content)\n",
    "        \n",
    "        # add \"\\n\" at the end\n",
    "        section1_content = section1_content + \"\\n\"\n",
    "        \n",
    "        summaries_leaflets.append(section1_content)\n",
    "    \n",
    "    return (content_plan_leaflets, summaries_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_records(records):\n",
    "    \"\"\"\n",
    "    To src_train.txt and src_valid.txt pre-append these special characters, according to data2text-plan project\n",
    "    \"\"\"\n",
    "    \n",
    "    record = []\n",
    "    record.append(UNK_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(BOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(EOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the src_train - training data to be input to the model\n",
    "\n",
    "def create_src_table(content_plan_leaflets):\n",
    "    \"\"\"\n",
    "    Create src_train - input \"table\" to the model\n",
    "    \n",
    "    Idea: - we do not have a table, so randomized the records in content plan\n",
    "    \n",
    "    Update: - do not randomize - make it easier for the model to learn\n",
    "    \"\"\"\n",
    "    \n",
    "    # store input \"table\" of each leaflet in array\n",
    "    src_leaflets = []\n",
    "    \n",
    "    for leaflet_content_plan in content_plan_leaflets:\n",
    "        # remove the end symbol ('\\n') of the string\n",
    "        leaflet_content_plan = leaflet_content_plan[:-2]\n",
    "\n",
    "        # split string into a list of records\n",
    "        leaflet_content_plan_collection = leaflet_content_plan.split(\" \")\n",
    "        \n",
    "        # do not do\n",
    "        # randomly shuffle records in a list\n",
    "        # random.shuffle(leaflet_content_plan_collection)\n",
    "        \n",
    "        # add special symbols to the begining\n",
    "        special_symbols = []\n",
    "        special_symbols = add_special_records(special_symbols)\n",
    "        \n",
    "        # create a string containing all the records and special_symbols in the begining\n",
    "        src_leaflet_section1 = ''\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(special_symbols)\n",
    "        src_leaflet_section1 += \" \"\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(leaflet_content_plan_collection)\n",
    "        src_leaflet_section1 += '\\n'\n",
    "\n",
    "\n",
    "        src_leaflets.append(src_leaflet_section1)\n",
    "    \n",
    "    return src_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE = 'src_train.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT = 'train_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_train, leaflets_inter_contentplan_train, leaflets_tgt_train = create_training_sets(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    src_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tgt_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    inter_train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "## Validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_sets(dataset):\n",
    "    \n",
    "    # Output files    \n",
    "    INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN_VALID, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_VALID, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_VALID, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT_VALID, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_valid, leaflets_inter_contentplan_valid, leaflets_tgt_valid = create_validation_sets(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/valid_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    contentplan_valid = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================\n",
    "## Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    SRC_FILE_TEST = 'test/src_test.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_TEST = \"test/tgt_test.txt\"  # tgt file of second stage \n",
    "    \n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to just summary and src data\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_TEST, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_TEST, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "\n",
    "    return src_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_test, leaflets_tgt_test = create_test_sets(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================\n",
    "## Creating *train-roto-ptrs.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "OUTPUT_FILE = \"train-roto-ptrs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_TGT_FILE) as reader:\n",
    "    leaflet_tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INTER_CONTENT_PLAN) as reader:\n",
    "    leaflets_inter_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg: the last entry 245,39 in train_roto_ptrs[1] indicates that the 245th token in summary matches with 39th content plan entry.  \n",
    "\n",
    "Phoenix ----> Phoenix￨Suns￨TEAM-CITY￨HOME  \n",
    "Suns ----> Suns￨Suns￨TEAM-NAME￨HOME  \n",
    "39 ----> 39￨Suns￨TEAM-WINS￨HOME  \n",
    "38 ----> 38￨Suns￨TEAM-LOSSES￨HOME  \n",
    "87 ----> 87￨Suns￨TEAM-PTS￨HOME  \n",
    "85 ----> 85￨Jazz￨TEAM-PTS￨AWAY  \n",
    "Utah ----> Utah￨Jazz￨TEAM-CITY￨AWAY  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cystinosis is a metabolic disease called ' nephropathic cystinosis ' which is characterized by an abnormal accumulation of the amino acid cystine in various organs of the body such as the kidney , eye , muscle , pancreas , and brain . cystine build up causes kidney damage and excretion of excess amounts of glucose , proteins and electrolytes . different organs are affected at different ages . cystagon is prescribed to manage this rare inherited disorder . cystagon is a medicine that reacts with cystine to decrease its level within the cells .\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cystagon￨Section1￨PRODUCT_NAME￨AWAY cystinosis￨Section1￨DX_NAME￨AWAY a_metabolic_disease￨Section1￨PROBLEM￨AWAY 'nephropathic_cystinosis'￨Section1￨PROBLEM￨AWAY an_abnormal_accumulation_of_the_amino_acid_cystine_in_various_organs_of_the_body_such_as_the_kidney,_eye,_muscle,_pancreas,_and_brain￨Section1￨PROBLEM￨AWAY cystine_build￨Section1￨PROBLEM￨AWAY kidney_damage￨Section1￨DX_NAME￨HOME excess_amounts_of_glucose￨Section1￨PROBLEM￨AWAY proteins￨Section1￨TEST_NAME￨HOME electrolytes￨Section1￨TEST_NAME￨AWAY cystagon￨Section1￨TREATMENT￨HOME this_rare_inherited_disorder￨Section1￨PROBLEM￨AWAY a_medicine￨Section1￨TREATMENT￨AWAY cystine￨Section1￨TREATMENT￨HOME \\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_pts_content = []\n",
    "\n",
    "# for each leaflet\n",
    "for leflet_num in range(len(leaflet_tgt_train)):\n",
    "    \n",
    "    # get current leaflet and content plan\n",
    "    current_leaflet = leaflet_tgt_train[leflet_num].split()\n",
    "    current_content_plan = leaflets_inter_content_plan[leflet_num].split()\n",
    "    \n",
    "    # get the values of content plan\n",
    "    instances = []\n",
    "    for entry in current_content_plan:\n",
    "        record_values = entry.split(DELIM)[0]\n",
    "        instances.append(record_values)\n",
    "    \n",
    "    # pairs (index_tgt, index_contentplan) for each leaflet\n",
    "    current_str = []\n",
    "    \n",
    "    # for each token in current summary\n",
    "    for token_pos in range(len(current_leaflet)):\n",
    "        \n",
    "        # get token\n",
    "        token = current_leaflet[token_pos]\n",
    "        \n",
    "        # possible tokens if 2 words in content plan like 'immunodeficiency_syndrome'\n",
    "        if token_pos < (len(current_leaflet)-1):\n",
    "            token_2words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1]\n",
    "        else:\n",
    "            token_2words = 'something that would never be in the section content'\n",
    "        \n",
    "        ### my-new-change\n",
    "        # possible tokens if 3 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-2):\n",
    "            token_3words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2]\n",
    "        else:\n",
    "            token_3words = 'something that would never be in the section content'\n",
    "        \n",
    "        # possible tokens if 4 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-3):\n",
    "            token_4words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2] + \"_\" + current_leaflet[token_pos+3]\n",
    "        else:\n",
    "            token_4words = 'something that would never be in the section content'\n",
    "        \n",
    "        \n",
    "        for content_plan_index in range(len(instances)):\n",
    "                \n",
    "            if token_4words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_3words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_2words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "    \n",
    "    # join pairs into string with \" \" between pairs\n",
    "    current_str = \" \".join(current_str)\n",
    "    \n",
    "    # add \"\\n\" at the end\n",
    "    current_str += \"\\n\"\n",
    "    \n",
    "    roto_pts_content.append(current_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"train-roto-ptrs.txt\"\n",
    "\n",
    "src_file = open(OUTPUT_FILE, 'w')\n",
    "for src_instance in roto_pts_content:\n",
    "    src_file.write(src_instance)\n",
    "src_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,0 7,3 14,4 26,5 34,6 39,7 51,8 56,9 57,10 60,20 67,11 96,12 99,13 101,14 103,15 110,16 112,17 121,18 123,19 134,21 136,22\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roto_pts_content[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content[index].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaflets_inter_content_plan[index].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zyclara', '3', '.', '75'] ----> zyclara￨Section1￨PRODUCT_NAME￨HOME\n",
      "['the', 'active', 'substance', 'imiquimod'] ----> the_active_substance_imiquimod￨Section1￨TREATMENT￨AWAY\n",
      "['an', 'immune', 'response', 'modifier'] ----> an_immune_response_modifier￨Section1￨TREATMENT￨AWAY\n",
      "['this', 'medicine', 'is', 'prescribed'] ----> this_medicine￨Section1￨TREATMENT￨AWAY\n",
      "['actinic', 'keratosis', 'in', 'adults'] ----> actinic_keratosis￨Section1￨DX_NAME￨HOME\n",
      "['this', 'medicine', 'stimulates', 'your'] ----> this_medicine￨Section1￨TREATMENT￨AWAY\n",
      "['natural', 'substances', 'which', 'help'] ----> natural_substances￨Section1￨TREATMENT￨AWAY\n",
      "['your', 'actinic', 'keratosis', '.'] ----> your_actinic_keratosis￨Section1￨PROBLEM￨HOME\n",
      "['actinic', 'keratosis', '.', 'actinic'] ----> actinic_keratosis￨Section1￨DX_NAME￨HOME\n",
      "['actinic', 'keratosis', 'appears', 'as'] ----> actinic_keratosis￨Section1￨DX_NAME￨AWAY\n",
      "['skin', 'found', 'in', 'people'] ----> skin￨Section1￨SYSTEM_ORGAN_SITE￨HOME\n",
      "['skin', 'or', 'are', 'greyish'] ----> skin￨Section1￨SYSTEM_ORGAN_SITE￨HOME\n",
      "['greyish', ',', 'pink', ','] ----> greyish￨Section1￨DX_NAME￨AWAY\n",
      "['pink', ',', 'red', 'or'] ----> pink￨Section1￨DX_NAME￨HOME\n",
      "['red', 'or', 'brown', '.'] ----> red￨Section1￨DX_NAME￨HOME\n",
      "['flat', 'and', 'scaly', ','] ----> flat￨Section1￨DX_NAME￨AWAY\n",
      "['scaly', ',', 'or', 'raised'] ----> scaly￨Section1￨DX_NAME￨HOME\n",
      "['warty', '.', 'this', 'medicine'] ----> warty￨Section1￨DX_NAME￨HOME\n",
      "['this', 'medicine', 'should', 'only'] ----> this_medicine￨Section1￨TREATMENT￨AWAY\n",
      "['face', 'or', 'scalp', 'if'] ----> face￨Section1￨SYSTEM_ORGAN_SITE￨AWAY\n",
      "['scalp', 'if', 'your', 'doctor'] ----> scalp￨Section1￨SYSTEM_ORGAN_SITE￨HOME\n"
     ]
    }
   ],
   "source": [
    "content_plan_indeces = []\n",
    "\n",
    "for pair in roto_pts_content[index].split():\n",
    "    pair = pair.split(\",\")\n",
    "    \n",
    "    a = int(pair[0])\n",
    "    b = int(pair[1])\n",
    "    \n",
    "    content_plan_indeces.append(b)\n",
    "    \n",
    "    print(leaflet_tgt_train[index].split()[a:a+4], end=\" ----> \")\n",
    "    print(leaflets_inter_content_plan[index].split()[b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zyclara￨Section1￨BRAND_NAME￨HOME\n",
      "3.75￨Section1￨NUMBER￨HOME\n"
     ]
    }
   ],
   "source": [
    "# check out pairs missed\n",
    "for i in range(0, len(leaflets_inter_content_plan[index].split()), 1):\n",
    "    if i not in content_plan_indeces:\n",
    "        print(leaflets_inter_content_plan[index].split()[i])\n",
    "        \n",
    "# explanation - 3-word long token\n",
    "# explanation - hiv_infection ---> bc NER outputs - 'hiv', 'hiv_infection' - in content plan I have 2 tokens starting with 'hiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"zyclara 3 . 75 % cream contains the active substance imiquimod , which is an immune response modifier ( to stimulate the human immune system ). this medicine is prescribed for the treatment of actinic keratosis in adults . this medicine stimulates your body ' s own immune system to produce natural substances which help fight your actinic keratosis . actinic keratosis appears as rough areas of skin found in people who have been exposed to a lot of sunshine over the course of their lifetime . these areas can be the same colour as your skin or are greyish , pink , red or brown . they can be flat and scaly , or raised , rough , hard and warty . this medicine should only be used for actinic keratosis on the face or scalp if your doctor has decided that it is the most appropriate treatment for you .\\n\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zyclara￨Section1￨PRODUCT_NAME￨HOME zyclara￨Section1￨BRAND_NAME￨HOME 3.75￨Section1￨NUMBER￨HOME the_active_substance_imiquimod￨Section1￨TREATMENT￨AWAY an_immune_response_modifier￨Section1￨TREATMENT￨AWAY this_medicine￨Section1￨TREATMENT￨AWAY actinic_keratosis￨Section1￨DX_NAME￨HOME this_medicine￨Section1￨TREATMENT￨AWAY natural_substances￨Section1￨TREATMENT￨AWAY your_actinic_keratosis￨Section1￨PROBLEM￨HOME actinic_keratosis￨Section1￨DX_NAME￨HOME skin￨Section1￨SYSTEM_ORGAN_SITE￨HOME skin￨Section1￨SYSTEM_ORGAN_SITE￨HOME greyish￨Section1￨DX_NAME￨AWAY pink￨Section1￨DX_NAME￨HOME red￨Section1￨DX_NAME￨HOME flat￨Section1￨DX_NAME￨AWAY scaly￨Section1￨DX_NAME￨HOME warty￨Section1￨DX_NAME￨HOME this_medicine￨Section1￨TREATMENT￨AWAY actinic_keratosis￨Section1￨DX_NAME￨AWAY face￨Section1￨SYSTEM_ORGAN_SITE￨AWAY scalp￨Section1￨SYSTEM_ORGAN_SITE￨HOME \\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
