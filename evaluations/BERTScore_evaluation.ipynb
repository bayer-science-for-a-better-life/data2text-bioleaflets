{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:   \n",
    "\n",
    "https://github.com/Tiiiger/bert_score     \n",
    "https://arxiv.org/pdf/1904.09675.pdf      \n",
    "\n",
    "Usage Examples:    \n",
    "https://github.com/Tiiiger/bert_score/blob/master/example/Demo.ipynb     \n",
    "https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import transformers\n",
    "\n",
    "# visuaization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"xtick.major.size\"] = 0\n",
    "rcParams[\"xtick.minor.size\"] = 0\n",
    "rcParams[\"ytick.major.size\"] = 0\n",
    "rcParams[\"ytick.minor.size\"] = 0\n",
    "\n",
    "rcParams[\"axes.labelsize\"] = \"large\"\n",
    "rcParams[\"axes.axisbelow\"] = True\n",
    "rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# check bert_score installation\n",
    "import bert_score\n",
    "from bert_score import BERTScorer\n",
    "bert_score.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GENERATED=\"/home/ruslan_yermakov/nlg-ra/T5_experiments/NLPcircle_model/outputs/mh_test_generations_explicit_path_model.txt\"\n",
    "PATH_ORIGINAL=\"/home/ruslan_yermakov/nlg-ra/T5_experiments/NLPcircle_model/input_data/test.target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read candidates\n",
    "with open(PATH_GENERATED) as f:\n",
    "    cands = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read references\n",
    "with open(PATH_ORIGINAL) as f:\n",
    "    refs = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(cands) == len(refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, most of the time of calling the `score` function is spent on building the model. In situations when we want to call the `score` function repeatedly, it is better to cache the model in a `scorer` object. Hence, in `bert_score` we also provide an object-oriented API. \n",
    "\n",
    "The `BERTScorer` class provides the two methods we have introduced above, `score` and `plot_example`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to score are a list of candidate sentences and a list of reference sentences.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some contextual embedding models, like RoBERTa, often produce BERTScores in a very narrow range (as shown above, the range is roughly between 0.92 and 1). Although this artifact does not affect the ranking ability of BERTScore, it affects the readability. Therefore, we propose to apply \"baseline rescaling\" to adjust the output scores. More details on this feature can be found in this post.   \n",
    "\n",
    "https://github.com/Tiiiger/bert_score/blob/master/journal/rescale_baseline.md   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to call the score function. Besides candidates and references, we need to speicify the bert model we are using. Since we are dealing with English sentences, we will use the bert-base-uncased model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, F1_score = scorer.score(cands, refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The outputs of the score function are Tensors of precision, recall, and F1 respectively. Each Tensor has the same number of items with the candidate and reference lists. Each item in the list is a scalar, representing the score for the corresponding candidates and references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: - truncates samples really long ??? but is it a problem? it should also truncate the original section, right??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System level F1 score: 0.337\n"
     ]
    }
   ],
   "source": [
    "print(f\"System level F1 score: {F1_score.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the scores are much more spread out, which makes it easy to compare different examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualisation\n",
    "# scorer.plot_example(cands[0], refs[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_text2text",
   "language": "python",
   "name": "t5_text2text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
