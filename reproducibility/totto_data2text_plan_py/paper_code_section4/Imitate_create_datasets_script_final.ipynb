{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TRAIN_DATASET.pickle\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_VALID_DATASET.pickle\", \"rb\") as f:\n",
    "    valid_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TEST_DATASET.pickle\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce same output as the script *create_dataset* from data2text-plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "section_content -------- true text   \n",
    "\n",
    "entity_recognition  ------- actually input - set of records   \n",
    "\n",
    "Set of records in order they appear in section_content    -------- content plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_DELIM = \" \"\n",
    "DELIM = u\"ï¿¨\"\n",
    "\n",
    "HOME = \"HOME\"\n",
    "AWAY = \"AWAY\"\n",
    "\n",
    "# ENTITY = \"Indication\"\n",
    "ENTITY = \"Section4\"\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure - sorted entities\n",
    "def _sort_key(entity):\n",
    "    return entity['BeginOffset']\n",
    "\n",
    "def test_order_entities(section_entities):\n",
    "    \n",
    "    sorted_entities = sorted(section_entities, key=_sort_key)\n",
    "    \n",
    "    if sorted_entities == section_entities: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def test_order_dataset(dataset):\n",
    "    for leaflet in dataset:\n",
    "\n",
    "        current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                    leaflet.section3, leaflet.section4, \n",
    "                                    leaflet.section5, leaflet.section6]\n",
    "\n",
    "        for current_section in current_leaflet_sections:\n",
    "\n",
    "            if current_section.entity_recognition is None:\n",
    "                continue\n",
    "\n",
    "            assert test_order_entities(current_section.entity_recognition) == True\n",
    "\n",
    "            \n",
    "test_order_dataset(train_dataset)\n",
    "test_order_dataset(valid_dataset)\n",
    "test_order_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_contentplan(dataset):\n",
    "    \"\"\"\n",
    "    Transform dataset to be a suitable format for model data2text-plan\n",
    "    \"\"\"\n",
    "    \n",
    "    # array to store section1 of each leaflet\n",
    "    summaries_leaflets = []\n",
    "    \n",
    "    # array to store content plan of each leaflet\n",
    "    content_plan_leaflets = []\n",
    "    \n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        # extract section1 content\n",
    "        section1_content = leaflet.section4.section_content\n",
    "        # extract results of NER\n",
    "        section1_entity_recognition = leaflet.section4.entity_recognition\n",
    "        \n",
    "        # skip if either input or output is None\n",
    "        if section1_content is None or section1_entity_recognition is None:\n",
    "            continue\n",
    "        \n",
    "        # get the content plan of each section\n",
    "        content_plan_section1 = ''\n",
    "\n",
    "        for entity in section1_entity_recognition:\n",
    "            entity_value = entity['Text'] if len(entity['Text'].split(\" \")) == 0 else (\"_\").join(entity['Text'].split(\" \"))\n",
    "            entity_type = entity['Type'] if entity['Type'] is not None and len(entity['Type']) > 0 else entity['Category']\n",
    "            \n",
    "            # randomly choose HOME or AWAY\n",
    "            if random.randint(1,2) == 1:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + HOME\n",
    "            else:\n",
    "                content_plan_section1 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + AWAY\n",
    "            \n",
    "            if section1_entity_recognition.index(entity) != len(section1_entity_recognition) - 1:\n",
    "                content_plan_section1 += \" \"\n",
    "            else:\n",
    "                content_plan_section1 += \" \" + \"\\n\"\n",
    "\n",
    "\n",
    "        content_plan_leaflets.append(content_plan_section1)\n",
    "\n",
    "        # get the section1 content\n",
    "        # make sure to have punctuations as a separate token\n",
    "        section1_content = wordpunct_tokenize(section1_content)\n",
    "        \n",
    "        # back to string\n",
    "        section1_content = \" \".join(section1_content)\n",
    "        \n",
    "        # add \"\\n\" at the end\n",
    "        section1_content = section1_content + \"\\n\"\n",
    "        \n",
    "        summaries_leaflets.append(section1_content)\n",
    "    \n",
    "    return (content_plan_leaflets, summaries_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_records(records):\n",
    "    \"\"\"\n",
    "    To src_train.txt and src_valid.txt pre-append these special characters, according to data2text-plan project\n",
    "    \"\"\"\n",
    "    \n",
    "    record = []\n",
    "    record.append(UNK_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(BOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(EOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the src_train - training data to be input to the model\n",
    "\n",
    "def create_src_table(content_plan_leaflets):\n",
    "    \"\"\"\n",
    "    Create src_train - input \"table\" to the model\n",
    "    \n",
    "    Idea: - we do not have a table, so randomized the records in content plan\n",
    "    \n",
    "    Update: - do not randomize - make it easier for the model to learn\n",
    "    \"\"\"\n",
    "    \n",
    "    # store input \"table\" of each leaflet in array\n",
    "    src_leaflets = []\n",
    "    \n",
    "    for leaflet_content_plan in content_plan_leaflets:\n",
    "        # remove the end symbol ('\\n') of the string\n",
    "        leaflet_content_plan = leaflet_content_plan[:-2]\n",
    "\n",
    "        # split string into a list of records\n",
    "        leaflet_content_plan_collection = leaflet_content_plan.split(\" \")\n",
    "        \n",
    "        # do not do\n",
    "        # randomly shuffle records in a list\n",
    "        # random.shuffle(leaflet_content_plan_collection)\n",
    "        \n",
    "        # add special symbols to the begining\n",
    "        special_symbols = []\n",
    "        special_symbols = add_special_records(special_symbols)\n",
    "        \n",
    "        # create a string containing all the records and special_symbols in the begining\n",
    "        src_leaflet_section1 = ''\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(special_symbols)\n",
    "        src_leaflet_section1 += \" \"\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(leaflet_content_plan_collection)\n",
    "        src_leaflet_section1 += '\\n'\n",
    "\n",
    "\n",
    "        src_leaflets.append(src_leaflet_section1)\n",
    "    \n",
    "    return src_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE = 'src_train.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT = 'train_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_train, leaflets_inter_contentplan_train, leaflets_tgt_train = create_training_sets(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    src_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tgt_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    inter_train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "## Validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_sets(dataset):\n",
    "    \n",
    "    # Output files    \n",
    "    INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN_VALID, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_VALID, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_VALID, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT_VALID, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_valid, leaflets_inter_contentplan_valid, leaflets_tgt_valid = create_validation_sets(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/valid_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    contentplan_valid = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================\n",
    "## Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    SRC_FILE_TEST = 'test/src_test.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_TEST = \"test/tgt_test.txt\"  # tgt file of second stage \n",
    "    \n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to just summary and src data\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_TEST, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_TEST, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "\n",
    "    return src_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_test, leaflets_tgt_test = create_test_sets(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================\n",
    "## Creating *train-roto-ptrs.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "OUTPUT_FILE = \"train-roto-ptrs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_TGT_FILE) as reader:\n",
    "    leaflet_tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INTER_CONTENT_PLAN) as reader:\n",
    "    leaflets_inter_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg: the last entry 245,39 in train_roto_ptrs[1] indicates that the 245th token in summary matches with 39th content plan entry.  \n",
    "\n",
    "Phoenix ----> Phoenixï¿¨Sunsï¿¨TEAM-CITYï¿¨HOME  \n",
    "Suns ----> Sunsï¿¨Sunsï¿¨TEAM-NAMEï¿¨HOME  \n",
    "39 ----> 39ï¿¨Sunsï¿¨TEAM-WINSï¿¨HOME  \n",
    "38 ----> 38ï¿¨Sunsï¿¨TEAM-LOSSESï¿¨HOME  \n",
    "87 ----> 87ï¿¨Sunsï¿¨TEAM-PTSï¿¨HOME  \n",
    "85 ----> 85ï¿¨Jazzï¿¨TEAM-PTSï¿¨AWAY  \n",
    "Utah ----> Utahï¿¨Jazzï¿¨TEAM-CITYï¿¨AWAY  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"like all medicines , cystagon can cause side effects , although not everybody gets them . cystagon may cause some people to become drowsy or less vigilant than they are normally . make sure you know how you or your child reacts to this medicine before doing anything that could be dangerous if not alert . the following side effects were reported as follows : very common ( occurring in at least one in 10 patients ), common ( occurring in at least one in 100 patients ), uncommon ( occurring in at least one in1 , 000 patients ), rare ( occurring in at least one in 10 , 000 patients ), very rare ( occurring in at least one in 100 , 000 patients ). - very common : vomiting , nausea , diarrhoea , loss of appetite , fever and sensation of sleep - common : abdominal pain or discomfort , unpleasant breath and body odour , skin eruption , gastroenteritis , fatigue , headache , encephalopathy ( brain disorder ) and liver function test abnormalities . - uncommon : skin striae , skin lesion ( little - hard lumps on elbows ), joint laxity , leg pain , bone fracture , scoliosis ( deviation of the vertebral column ), bone deformity and fragility , hair discoloration , severe allergic reaction , somnolence , fits , nervousness , hallucination , decrease in white blood cells , gastrointestinal ulcer manifested by bleeding in the digestive tract and effect on the kidney manifested by swelling of the extremities and weight gain . since some of these side effects are serious , ask your or your child ' s doctor to explain their warning signs . reporting of side effects if you get any side effects , talk to your doctor or pharmacist . this includes any possible side effects not listed in this leaflet . you can also report side effects directly via the national reporting system listed in appendix v . by reporting side effects you can help provide more information on the safety of this medicine .\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cystagonï¿¨Section4ï¿¨PRODUCT_NAMEï¿¨AWAY all_medicinesï¿¨Section4ï¿¨TREATMENTï¿¨HOME cystagonï¿¨Section4ï¿¨GENERIC_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY cystagonï¿¨Section4ï¿¨GENERIC_NAMEï¿¨AWAY drowsyï¿¨Section4ï¿¨DX_NAMEï¿¨HOME alertï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME 10ï¿¨Section4ï¿¨NUMBERï¿¨HOME 100ï¿¨Section4ï¿¨NUMBERï¿¨HOME 10,000ï¿¨Section4ï¿¨NUMBERï¿¨AWAY 100,000ï¿¨Section4ï¿¨NUMBERï¿¨AWAY vomitingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY nauseaï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY diarrhoeaï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY loss_of_appetiteï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY feverï¿¨Section4ï¿¨DX_NAMEï¿¨HOME sensation_of_sleepï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY abdominal_painï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY discomfortï¿¨Section4ï¿¨DX_NAMEï¿¨HOME unpleasant_breathï¿¨Section4ï¿¨DX_NAMEï¿¨HOME body_odourï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY skin_eruptionï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY gastroenteritisï¿¨Section4ï¿¨DX_NAMEï¿¨HOME fatigueï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY headacheï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY encephalopathy_(brain_disorderï¿¨Section4ï¿¨PROBLEMï¿¨AWAY liver_function_test_abnormalitiesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY skin_striaeï¿¨Section4ï¿¨DX_NAMEï¿¨HOME skin_lesionï¿¨Section4ï¿¨DX_NAMEï¿¨HOME hard_lumps_on_elbowsï¿¨Section4ï¿¨PROBLEMï¿¨HOME joint_laxityï¿¨Section4ï¿¨DX_NAMEï¿¨HOME leg_painï¿¨Section4ï¿¨DX_NAMEï¿¨HOME bone_fractureï¿¨Section4ï¿¨DX_NAMEï¿¨HOME scoliosis_(deviation_of_the_vertebral_columnï¿¨Section4ï¿¨PROBLEMï¿¨HOME bone_deformityï¿¨Section4ï¿¨DX_NAMEï¿¨HOME fragilityï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY hair_discolorationï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY severe_allergic_reactionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY somnolenceï¿¨Section4ï¿¨DX_NAMEï¿¨HOME nervousnessï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY hallucinationï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY decrease_in_white_blood_cellsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME gastrointestinal_ulcerï¿¨Section4ï¿¨DX_NAMEï¿¨HOME bleeding_in_the_digestive_tractï¿¨Section4ï¿¨PROBLEMï¿¨HOME the_kidneyï¿¨Section4ï¿¨PROBLEMï¿¨HOME swelling_of_the_extremitiesï¿¨Section4ï¿¨PROBLEMï¿¨HOME weight_gainï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME appendix_v.ï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨AWAY \\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_pts_content = []\n",
    "\n",
    "# for each leaflet\n",
    "for leflet_num in range(len(leaflet_tgt_train)):\n",
    "    \n",
    "    # get current leaflet and content plan\n",
    "    current_leaflet = leaflet_tgt_train[leflet_num].split()\n",
    "    current_content_plan = leaflets_inter_content_plan[leflet_num].split()\n",
    "    \n",
    "    # get the values of content plan\n",
    "    instances = []\n",
    "    for entry in current_content_plan:\n",
    "        record_values = entry.split(DELIM)[0]\n",
    "        instances.append(record_values)\n",
    "    \n",
    "    # pairs (index_tgt, index_contentplan) for each leaflet\n",
    "    current_str = []\n",
    "    \n",
    "    # for each token in current summary\n",
    "    for token_pos in range(len(current_leaflet)):\n",
    "        \n",
    "        # get token\n",
    "        token = current_leaflet[token_pos]\n",
    "        \n",
    "        # possible tokens if 2 words in content plan like 'immunodeficiency_syndrome'\n",
    "        if token_pos < (len(current_leaflet)-1):\n",
    "            token_2words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1]\n",
    "        else:\n",
    "            token_2words = 'something that would never be in the section content'\n",
    "        \n",
    "        ### my-new-change\n",
    "        # possible tokens if 3 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-2):\n",
    "            token_3words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2]\n",
    "        else:\n",
    "            token_3words = 'something that would never be in the section content'\n",
    "        \n",
    "        # possible tokens if 4 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-3):\n",
    "            token_4words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2] + \"_\" + current_leaflet[token_pos+3]\n",
    "        else:\n",
    "            token_4words = 'something that would never be in the section content'\n",
    "        \n",
    "        \n",
    "        for content_plan_index in range(len(instances)):\n",
    "                \n",
    "            if token_4words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_3words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_2words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "    \n",
    "    # join pairs into string with \" \" between pairs\n",
    "    current_str = \" \".join(current_str)\n",
    "    \n",
    "    # add \"\\n\" at the end\n",
    "    current_str += \"\\n\"\n",
    "    \n",
    "    roto_pts_content.append(current_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"train-roto-ptrs.txt\"\n",
    "\n",
    "src_file = open(OUTPUT_FILE, 'w')\n",
    "for src_instance in roto_pts_content:\n",
    "    src_file.write(src_instance)\n",
    "src_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,1 4,2 8,3 19,4 24,5 27,6 28,31 33,7 36,8 38,9 41,10 42,11 53,0 58,13 63,14 65,15 67,16 72,17 81,18 84,19 92,20 94,21 99,22 103,23 107,24 109,25 112,26 115,27 118,54 123,28 133,29 147,32 152,92 166,93 172,33 175,34 177,35 180,36 183,38 185,37 191,88 198,12 205,40 207,41 209,42 214,44 228,47 230,48 240,49 242,50 249,51 273,39 281,55 285,56 287,57 289,58 293,59 298,60 301,61 308,62 311,63 313,64 316,65 318,66 320,67 322,68 326,69 329,70 330,90 333,71 336,72 338,73 367,77 383,80 387,81 396,82 401,84 408,85 428,89 437,91 443,94 449,95 463,97 485,96 502,98\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roto_pts_content[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content[index].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaflets_inter_content_plan[index].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'medicines', ',', 'this'] ----> all_medicinesï¿¨Section4ï¿¨TREATMENTï¿¨HOME\n",
      "['this', 'medicine', 'can', 'cause'] ----> this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨AWAY\n",
      "['side', 'effects', ',', 'although'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['side', 'effects', 'may', 'happen'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['this', 'medicine', ':', 'serious'] ----> this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME\n",
      "['serious', 'side', 'effects', 'uncommon'] ----> serious_side_effectsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['side', 'effects', 'uncommon', '('] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['affect', 'up', 'to', '1'] ----> affectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['1', 'in', '100', 'people'] ----> 1ï¿¨Section4ï¿¨NUMBERï¿¨AWAY\n",
      "['100', 'people', ')', 'immediate'] ----> 100ï¿¨Section4ï¿¨NUMBERï¿¨AWAY\n",
      "['immediate', 'breathing', 'difficulties', ':'] ----> immediate_breathing_difficultiesï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['breathing', 'difficulties', ':', 'if'] ----> breathing_difficultiesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['trixeo', 'aerosphere', ',', 'such'] ----> trixeo_aerosphereï¿¨Section4ï¿¨PRODUCT_NAMEï¿¨AWAY\n",
      "['tightness', 'of', 'the', 'chest'] ----> tightness_of_the_chestï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['coughing', ',', 'wheezing', 'or'] ----> coughingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['wheezing', 'or', 'feeling', 'breathless'] ----> wheezingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['feeling', 'breathless', ',', 'stop'] ----> feeling_breathlessï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['this', 'medicine', 'and', 'tell'] ----> this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨AWAY\n",
      "['allergic', 'reactions', ':', 'swelling'] ----> allergic_reactionsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['swelling', 'of', 'your', 'face'] ----> swelling_of_your_faceï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['mouth', '(', 'swelling', 'of'] ----> mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY\n",
      "['swelling', 'of', 'your', 'tongue'] ----> swelling_of_your_tongueï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['throat', 'may', 'make', 'it'] ----> throatï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME\n",
      "['difficult', 'to', 'swallow', ')'] ----> difficult_to_swallowï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['rash', 'or', 'hives', 'together'] ----> rashï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['hives', 'together', 'with', 'difficulty'] ----> hivesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['difficulty', 'breathing', 'suddenly', 'feeling'] ----> difficulty_breathingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['feeling', 'faint', 'these', 'symptoms'] ----> feeling_faintï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['symptoms', 'may', 'be', 'signs'] ----> symptomsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['an', 'allergic', 'reaction', 'which'] ----> an_allergic_reactionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['this', 'medicine', 'and', 'call'] ----> this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME\n",
      "['side', 'effects', 'above', '.'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['side', 'effects', 'tell', 'your'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['side', 'effects', ':', 'common'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['affect', 'up', 'to', '1'] ----> affectï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['1', 'in', '10', 'people'] ----> 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME\n",
      "['10', 'people', ')', 'thrush'] ----> 10ï¿¨Section4ï¿¨NUMBERï¿¨AWAY\n",
      "['thrush', 'in', 'the', 'mouth'] ----> thrush_in_the_mouthï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['mouth', '(', 'a', 'fungal'] ----> mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME\n",
      "['a', 'fungal', 'infection', ').'] ----> a_fungal_infectionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['mouth', 'out', 'with', 'water'] ----> mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY\n",
      "['trixeo', 'aerosphere', 'may', 'help'] ----> trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨AWAY\n",
      "['feeling', 'anxious', 'difficulty', 'sleeping'] ----> feeling_anxiousï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['difficulty', 'sleeping', 'feeling', 'sick'] ----> difficulty_sleepingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['feeling', 'sick', '(', 'nausea'] ----> feeling_sickï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['headache', 'coughing', 'or', 'a'] ----> headache_coughingï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['palpitations', ')', 'high', 'blood'] ----> palpitationsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['high', 'blood', 'sugar', 'levels'] ----> high_blood_sugar_levelsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['painful', 'and', 'frequent', 'urination'] ----> painfulï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['frequent', 'urination', '(', 'may'] ----> frequent_urinationï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['a', 'urinary', 'tract', 'infection'] ----> a_urinary_tract_infectionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['trixeo', 'aerosphere', ',', 'they'] ----> trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨HOME\n",
      "['a', 'lung', 'infection', ':'] ----> a_lung_infectionï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['fever', 'or', 'chills', ','] ----> feverï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['chills', ',', 'increased', 'mucus'] ----> chillsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['increased', 'mucus', 'production', ','] ----> increased_mucus_productionï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['change', 'in', 'mucus', 'colour'] ----> change_in_mucus_colourï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['increased', 'cough', 'or', 'increased'] ----> increased_coughï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['increased', 'breathing', 'difficulties', '.'] ----> increased_breathing_difficultiesï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['affect', 'up', 'to', '1'] ----> affectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['1', 'in', '100', 'people'] ----> 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME\n",
      "['100', 'people', ')', 'shaking'] ----> 100ï¿¨Section4ï¿¨NUMBERï¿¨HOME\n",
      "['shaking', ',', 'tremor', 'or'] ----> shakingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['tremor', 'or', 'feeling', 'dizzy'] ----> tremorï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['feeling', 'dizzy', 'dry', 'mouth'] ----> feeling_dizzyï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['dry', 'mouth', ',', 'or'] ----> dry_mouthï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['mild', 'irritation', 'in', 'the'] ----> mild_irritationï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['the', 'throat', 'bruising', 'of'] ----> the_throat_bruisingï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "['throat', 'bruising', 'of', 'the'] ----> throatï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY\n",
      "['the', 'skin', 'feeling', 'restless'] ----> the_skinï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['restless', ',', 'nervous', 'or'] ----> restlessï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['nervous', 'or', 'agitated', 'depression'] ----> nervousï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['1', 'in', '10', ','] ----> 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME\n",
      "['35', 'not', 'known', '('] ----> 35ï¿¨Section4ï¿¨NUMBERï¿¨AWAY\n",
      "['frequency', 'cannot', 'be', 'estimated'] ----> frequencyï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['blurred', 'vision', 'clouding', 'of'] ----> blurred_visionï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['lens', 'of', 'your', 'eyes'] ----> lensï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME\n",
      "['cataract', ')', 'increased', 'pressure'] ----> cataractï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['swelling', 'of', 'your', 'tongue'] ----> swelling_of_your_tongueï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "['difficult', 'to', 'swallow', ')'] ----> difficult_to_swallowï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['side', 'effects', 'if', 'you'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['side', 'effects', ',', 'talk'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME\n",
      "['side', 'effects', 'not', 'listed'] ----> side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "['appendix', 'v', '.', 'by'] ----> appendixï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME\n",
      "['this', 'medicine', '.'] ----> this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME\n"
     ]
    }
   ],
   "source": [
    "content_plan_indeces = []\n",
    "\n",
    "for pair in roto_pts_content[index].split():\n",
    "    pair = pair.split(\",\")\n",
    "    \n",
    "    a = int(pair[0])\n",
    "    b = int(pair[1])\n",
    "    \n",
    "    content_plan_indeces.append(b)\n",
    "    \n",
    "    print(leaflet_tgt_train[index].split()[a:a+4], end=\" ----> \")\n",
    "    print(leaflets_inter_content_plan[index].split()[b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_serious_side_effects_aboveï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "nausea)ï¿¨Section4ï¿¨PROBLEMï¿¨HOME\n",
      "a_hoarse_voice_muscle_crampsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "awareness_of_your_heart_beatingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "pneumonia_(infection_of_the_lungï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨AWAY\n",
      "agitated_depression_fast_heart_beatï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "uneven_heart_beat_chest_painï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "tightening_in_the_chest_(angina_pectoris)ï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "10,000ï¿¨Section4ï¿¨NUMBERï¿¨AWAY\n",
      "changes_in_behaviour_an_effectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "clouding_of_the_lens_of_your_eyesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY\n",
      "increased_pressure_in_the_eyeï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n",
      "glaucoma)_swelling_of_your_faceï¿¨Section4ï¿¨PROBLEMï¿¨AWAY\n"
     ]
    }
   ],
   "source": [
    "# check out pairs missed\n",
    "for i in range(0, len(leaflets_inter_content_plan[index].split()), 1):\n",
    "    if i not in content_plan_indeces:\n",
    "        print(leaflets_inter_content_plan[index].split()[i])\n",
    "        \n",
    "# explanation - 3-word long token\n",
    "# explanation - hiv_infection ---> bc NER outputs - 'hiv', 'hiv_infection' - in content plan I have 2 tokens starting with 'hiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like all medicines , this medicine can cause side effects , although not everybody gets them . the following side effects may happen with this medicine : serious side effects uncommon ( may affect up to 1 in 100 people ) immediate breathing difficulties : if you get breathing difficulties straight after using trixeo aerosphere , such as tightness of the chest , coughing , wheezing or feeling breathless , stop using this medicine and tell your doctor straight away . allergic reactions : swelling of your face , particularly around your mouth ( swelling of your tongue or throat may make it difficult to swallow ) rash or hives together with difficulty breathing suddenly feeling faint these symptoms may be signs of an allergic reaction which may become serious . stop using this medicine and call for medical help straight away if you notice the serious side effects above . other side effects tell your doctor or pharmacist if you notice any of the following side effects : common ( may affect up to 1 in 10 people ) thrush in the mouth ( a fungal infection ). rinsing your mouth out with water immediately after using trixeo aerosphere may help prevent this . feeling anxious difficulty sleeping feeling sick ( nausea ) headache coughing or a hoarse voice muscle cramps awareness of your heart beating ( palpitations ) high blood sugar levels ( as shown in tests ) painful and frequent urination ( may be signs of a urinary tract infection ) pneumonia ( infection of the lung ). tell your doctor if you have any of the following while using trixeo aerosphere , they could be symptoms of a lung infection : fever or chills , increased mucus production , change in mucus colour , increased cough or increased breathing difficulties . uncommon ( may affect up to 1 in 100 people ) shaking , tremor or feeling dizzy dry mouth , or mild irritation in the throat bruising of the skin feeling restless , nervous or agitated depression fast heart beat or uneven heart beat chest pain or tightening in the chest ( angina pectoris ) very rare ( may affect up to 1 in 10 , 000 people ) changes in behaviour an effect on the adrenal gland 35 not known ( frequency cannot be estimated from the available data ): blurred vision clouding of the lens of your eyes ( signs of cataract ) increased pressure in the eye ( glaucoma ) swelling of your face , particularly around your mouth ( swelling of your tongue or throat may make it difficult to swallow ) reporting of side effects if you get any side effects , talk to your doctor or pharmacist . this includes any possible side effects not listed in this leaflet . you can also report side effects directly via the national reporting system listed in appendix v . by reporting side effects you can help provide more information on the safety of this medicine .\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trixeo_aerosphereï¿¨Section4ï¿¨PRODUCT_NAMEï¿¨AWAY all_medicinesï¿¨Section4ï¿¨TREATMENTï¿¨HOME this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME serious_side_effectsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY affectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY 1ï¿¨Section4ï¿¨NUMBERï¿¨AWAY 100ï¿¨Section4ï¿¨NUMBERï¿¨AWAY immediate_breathing_difficultiesï¿¨Section4ï¿¨PROBLEMï¿¨AWAY breathing_difficultiesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨AWAY tightness_of_the_chestï¿¨Section4ï¿¨PROBLEMï¿¨AWAY coughingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY wheezingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME feeling_breathlessï¿¨Section4ï¿¨DX_NAMEï¿¨HOME this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨AWAY allergic_reactionsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME swelling_of_your_faceï¿¨Section4ï¿¨PROBLEMï¿¨AWAY mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY swelling_of_your_tongueï¿¨Section4ï¿¨PROBLEMï¿¨HOME throatï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME difficult_to_swallowï¿¨Section4ï¿¨DX_NAMEï¿¨HOME rashï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY hivesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY difficulty_breathingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME feeling_faintï¿¨Section4ï¿¨DX_NAMEï¿¨HOME an_allergic_reactionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME the_serious_side_effects_aboveï¿¨Section4ï¿¨PROBLEMï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY affectï¿¨Section4ï¿¨DX_NAMEï¿¨HOME 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME 10ï¿¨Section4ï¿¨NUMBERï¿¨AWAY thrush_in_the_mouthï¿¨Section4ï¿¨PROBLEMï¿¨AWAY a_fungal_infectionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨HOME feeling_anxiousï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY difficulty_sleepingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY feeling_sickï¿¨Section4ï¿¨DX_NAMEï¿¨HOME nausea)ï¿¨Section4ï¿¨PROBLEMï¿¨HOME headache_coughingï¿¨Section4ï¿¨PROBLEMï¿¨AWAY a_hoarse_voice_muscle_crampsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY awareness_of_your_heart_beatingï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY palpitationsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY high_blood_sugar_levelsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME painfulï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY frequent_urinationï¿¨Section4ï¿¨DX_NAMEï¿¨HOME a_urinary_tract_infectionï¿¨Section4ï¿¨PROBLEMï¿¨AWAY pneumonia_(infection_of_the_lungï¿¨Section4ï¿¨PROBLEMï¿¨AWAY trixeo_aerosphereï¿¨Section4ï¿¨TREATMENTï¿¨AWAY symptomsï¿¨Section4ï¿¨PROBLEMï¿¨AWAY a_lung_infectionï¿¨Section4ï¿¨PROBLEMï¿¨HOME feverï¿¨Section4ï¿¨DX_NAMEï¿¨HOME chillsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME increased_mucus_productionï¿¨Section4ï¿¨PROBLEMï¿¨HOME change_in_mucus_colourï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY increased_coughï¿¨Section4ï¿¨PROBLEMï¿¨HOME increased_breathing_difficultiesï¿¨Section4ï¿¨PROBLEMï¿¨HOME affectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME 100ï¿¨Section4ï¿¨NUMBERï¿¨HOME shakingï¿¨Section4ï¿¨DX_NAMEï¿¨HOME tremorï¿¨Section4ï¿¨DX_NAMEï¿¨HOME feeling_dizzyï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY dry_mouthï¿¨Section4ï¿¨DX_NAMEï¿¨HOME mild_irritationï¿¨Section4ï¿¨PROBLEMï¿¨AWAY the_throat_bruisingï¿¨Section4ï¿¨PROBLEMï¿¨AWAY the_skinï¿¨Section4ï¿¨PROBLEMï¿¨HOME restlessï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY nervousï¿¨Section4ï¿¨DX_NAMEï¿¨HOME agitated_depression_fast_heart_beatï¿¨Section4ï¿¨PROBLEMï¿¨AWAY uneven_heart_beat_chest_painï¿¨Section4ï¿¨PROBLEMï¿¨AWAY tightening_in_the_chest_(angina_pectoris)ï¿¨Section4ï¿¨PROBLEMï¿¨AWAY 1ï¿¨Section4ï¿¨NUMBERï¿¨HOME 10,000ï¿¨Section4ï¿¨NUMBERï¿¨AWAY changes_in_behaviour_an_effectï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY 35ï¿¨Section4ï¿¨NUMBERï¿¨AWAY frequencyï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY blurred_visionï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY clouding_of_the_lens_of_your_eyesï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY lensï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME cataractï¿¨Section4ï¿¨DX_NAMEï¿¨HOME increased_pressure_in_the_eyeï¿¨Section4ï¿¨PROBLEMï¿¨AWAY glaucoma)_swelling_of_your_faceï¿¨Section4ï¿¨PROBLEMï¿¨AWAY mouthï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY swelling_of_your_tongueï¿¨Section4ï¿¨PROBLEMï¿¨HOME throatï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY difficult_to_swallowï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨HOME appendixï¿¨Section4ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME side_effectsï¿¨Section4ï¿¨DX_NAMEï¿¨AWAY this_medicineï¿¨Section4ï¿¨TREATMENTï¿¨HOME \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
