{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main goal - split dataset into train-val-test\n",
    "\n",
    "- Additionally remove sections that are outliers (too long length)   \n",
    "- Additionally add PRODUCT_NAME as the 1st entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# split into train-dev-test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"datasets/LEAFLET_DATASET_PROCESSED_NER_COMBINED.pickle\", \"rb\") as f:\n",
    "    package_leaflets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1336"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(package_leaflets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Leaflets dataset    \n",
    "\n",
    "- replace too short sections with None\n",
    "- set duplicate section_content to None \n",
    "- set duplicate entity_recognition to None \n",
    "- remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set empty sections to None\n",
    "# set duplicate section_content to None \n",
    "# set duplicate entity_recognition to None \n",
    "\n",
    "\n",
    "# keep track of unique NER outputs observed so far\n",
    "unique_NER_outputs = dict()\n",
    "\n",
    "# keep track of unique section contents observed so far\n",
    "unique_section_content = dict()\n",
    "\n",
    "COUNT_DUPLICATE_NER_OUTPUTS = 0\n",
    "COUNT_DUPLICATE_SECTION_CONTENT = 0\n",
    "\n",
    "\n",
    "for leaflet in package_leaflets:\n",
    "    \n",
    "    current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                leaflet.section3, leaflet.section4, \n",
    "                                leaflet.section5, leaflet.section6]\n",
    "    \n",
    "    for section_index, current_section in enumerate(current_leaflet_sections):\n",
    "        \n",
    "        # if section_content is None, make sure entity_recognition is None too (can not map NER --> None)\n",
    "        if current_section.section_content is None:\n",
    "            current_section.entity_recognition = None\n",
    "            continue\n",
    "        \n",
    "        # if entity_recognition is None, make sure section_content is None too (can not map from None --> text)\n",
    "        if current_section.entity_recognition is None:\n",
    "            current_section.section_content = None\n",
    "            continue\n",
    "        \n",
    "        # set empty section_content to None, make sure entity_recognition is None too (can not map NER --> None)\n",
    "        if len(current_section.section_content) == 0:\n",
    "            current_section.section_content = None\n",
    "            current_section.entity_recognition = None\n",
    "            continue\n",
    "        \n",
    "        # set empty NER outputs to None, make sure section_content is None too (can not map from None --> text)\n",
    "        if len(current_section.entity_recognition) == 0:\n",
    "            current_section.entity_recognition = None\n",
    "            current_section.section_content = None\n",
    "            continue\n",
    "            \n",
    "        ### set duplicate NER outputs to None\n",
    "        \n",
    "        is_duplicate_NER = False\n",
    "        \n",
    "        # get only the 'Text' of entities\n",
    "        current_section_entities = ''\n",
    "        for entity in current_section.entity_recognition:\n",
    "            current_section_entities += entity['Text'] + ' '\n",
    "        \n",
    "        if current_section_entities not in unique_NER_outputs:\n",
    "            unique_NER_outputs[current_section_entities] = 1\n",
    "        else:\n",
    "            unique_NER_outputs[current_section_entities] += 1\n",
    "            COUNT_DUPLICATE_NER_OUTPUTS += 1\n",
    "            is_duplicate_NER = True\n",
    "        \n",
    "        \n",
    "        ### set duplicate section content to None\n",
    "        \n",
    "        is_duplicate_section_content = False\n",
    "        \n",
    "        section_content = current_section.section_content\n",
    "        \n",
    "        if section_content not in unique_section_content:\n",
    "            unique_section_content[section_content] = 1\n",
    "        else:\n",
    "            unique_section_content[section_content] += 1\n",
    "            COUNT_DUPLICATE_SECTION_CONTENT += 1\n",
    "            is_duplicate_section_content = True\n",
    "        \n",
    "        \n",
    "        # set duplicate section_content or duplicate NER output to None\n",
    "        if (section_index+1) == 1:\n",
    "            if is_duplicate_NER: leaflet.section1.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section1.section_content = None\n",
    "        elif (section_index+1) == 2:\n",
    "            if is_duplicate_NER: leaflet.section2.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section2.section_content = None\n",
    "        elif (section_index+1) == 3:\n",
    "            if is_duplicate_NER: leaflet.section3.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section3.section_content = None\n",
    "        elif (section_index+1) == 4:\n",
    "            if is_duplicate_NER: leaflet.section4.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section4.section_content = None\n",
    "        elif (section_index+1) == 5:\n",
    "            if is_duplicate_NER: leaflet.section5.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section5.section_content = None\n",
    "        elif (section_index+1) == 6:\n",
    "            if is_duplicate_NER: leaflet.section6.entity_recognition = None\n",
    "            if is_duplicate_section_content: leaflet.section6.section_content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNT_DUPLICATE_NER_OUTPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNT_DUPLICATE_SECTION_CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "\n",
    "# calculate length of each section\n",
    "section1_content_length = []\n",
    "section2_content_length = []\n",
    "section3_content_length = []\n",
    "section4_content_length = []\n",
    "section5_content_length = []\n",
    "section6_content_length = []\n",
    "\n",
    "\n",
    "# calc the length of section content and add to list\n",
    "for leaflet_idx in range(len(package_leaflets)):\n",
    "    \n",
    "    for section_idx in range(1,7):\n",
    "        \n",
    "        if section_idx == 1:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section1.section_content\n",
    "            if current_section_content is not None: section1_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 2:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section2.section_content\n",
    "            if current_section_content is not None: section2_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 3:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section3.section_content\n",
    "            if current_section_content is not None: section3_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 4:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section4.section_content\n",
    "            if current_section_content is not None: section4_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 5:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section5.section_content\n",
    "            if current_section_content is not None: section5_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 6:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section6.section_content\n",
    "            if current_section_content is not None: section6_content_length.append(len(current_section_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  1005.2061933534743\n",
      "Section 2:  4654.529144587434\n",
      "Section 3:  2397.9171686746986\n",
      "Section 4:  3595.102056359482\n",
      "Section 5:  644.16\n",
      "Section 6:  1033.1423164269493\n"
     ]
    }
   ],
   "source": [
    "print('Section 1: ', np.mean(section1_content_length))\n",
    "print('Section 2: ', np.mean(section2_content_length))\n",
    "print('Section 3: ', np.mean(section3_content_length))\n",
    "print('Section 4: ', np.mean(section4_content_length))\n",
    "print('Section 5: ', np.mean(section5_content_length))\n",
    "print('Section 6: ', np.mean(section6_content_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_threshold(data, name='', m=3.5):\n",
    "    \"\"\"\n",
    "    Outliers are to the right side of the distribution\n",
    "    \n",
    "    Outliers:\n",
    "    print('Outliers - Section Lengths:', data[abs(data - np.mean(data)) > m * np.std(data)])\n",
    "    \n",
    "    Check:\n",
    "    print(data[abs(data - np.mean(data)) > m * np.std(data)] >= min(data[abs(data - np.mean(data)) > m * np.std(data)]))\n",
    "    \"\"\"\n",
    "    \n",
    "    # filtered data without outliers\n",
    "    filtered_data = data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "    \n",
    "    # outliers\n",
    "    outliers = data[abs(data - np.mean(data)) > m * np.std(data)]\n",
    "    \n",
    "    # print - number of outliers\n",
    "    print(name, len(data), '-', len(filtered_data), \"=\", len(outliers), '\\tThreshold:', min(outliers))\n",
    "        \n",
    "    # find the threshold, section content with length > threshold ---> outliers\n",
    "    return min(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section1: 1324 - 1314 = 10 \tThreshold: 3858\n",
      "Section2: 1321 - 1309 = 12 \tThreshold: 12766\n",
      "Section3: 1328 - 1313 = 15 \tThreshold: 8618\n",
      "Section4: 1313 - 1295 = 18 \tThreshold: 12048\n",
      "Section5: 1175 - 1172 = 3 \tThreshold: 4594\n",
      "Section6: 1321 - 1311 = 10 \tThreshold: 4233\n"
     ]
    }
   ],
   "source": [
    "outliers_threshold = {\n",
    "    '1': find_outliers_threshold(np.array(section1_content_length), name='Section1:'),\n",
    "    '2': find_outliers_threshold(np.array(section2_content_length), name='Section2:'),\n",
    "    '3': find_outliers_threshold(np.array(section3_content_length), name='Section3:'),\n",
    "    '4': find_outliers_threshold(np.array(section4_content_length), name='Section4:'),\n",
    "    '5': find_outliers_threshold(np.array(section5_content_length), name='Section5:'),\n",
    "    '6': find_outliers_threshold(np.array(section6_content_length), name='Section6:')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Before I used some hard-coded thresholds based on distribution analysis.  \n",
    "\n",
    "{'1': 4000, '2': 14000, '3': 9000, '4': 14000, '5': 2000, '6': 5000}     \n",
    "\n",
    "Section1: 1324 - 1316 = 8 \tThreshold: 4000     \n",
    "Section2: 1321 - 1316 = 5 \tThreshold: 14000     \n",
    "Section3: 1328 - 1319 = 9 \tThreshold: 9000     \n",
    "Section4: 1313 - 1307 = 6 \tThreshold: 14000     \n",
    "Section5: 1231 - 1228 = 3 \tThreshold: 2000     \n",
    "Section6: 1321 - 1316 = 5 \tThreshold: 5000       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(section1_content_length, bins=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set outliers section content to None\n",
    "\n",
    "for leaflet_idx in range(len(package_leaflets)):\n",
    "    \n",
    "    for section_idx in range(1,7):\n",
    "        \n",
    "        if section_idx == 1:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section1.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['1']: package_leaflets[leaflet_idx].section1.section_content = None\n",
    "        elif section_idx == 2:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section2.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['2']: package_leaflets[leaflet_idx].section2.section_content = None\n",
    "        elif section_idx == 3:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section3.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['3']: package_leaflets[leaflet_idx].section3.section_content = None\n",
    "        elif section_idx == 4:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section4.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['4']: package_leaflets[leaflet_idx].section4.section_content = None\n",
    "        elif section_idx == 5:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section5.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['5']: package_leaflets[leaflet_idx].section5.section_content = None\n",
    "        elif section_idx == 6:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section6.section_content\n",
    "            if current_section_content is not None and len(current_section_content) >= outliers_threshold['6']: package_leaflets[leaflet_idx].section6.section_content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the mean length of each section after removing outliers\n",
    "\n",
    "# calculate length of each section\n",
    "section1_content_length = []\n",
    "section2_content_length = []\n",
    "section3_content_length = []\n",
    "section4_content_length = []\n",
    "section5_content_length = []\n",
    "section6_content_length = []\n",
    "\n",
    "\n",
    "# calc the length of section content and add to list\n",
    "\n",
    "for leaflet_idx in range(len(package_leaflets)):\n",
    "    \n",
    "    for section_idx in range(1,7):\n",
    "        \n",
    "        if section_idx == 1:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section1.section_content\n",
    "            if current_section_content is not None: section1_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 2:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section2.section_content\n",
    "            if current_section_content is not None: section2_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 3:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section3.section_content\n",
    "            if current_section_content is not None: section3_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 4:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section4.section_content\n",
    "            if current_section_content is not None: section4_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 5:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section5.section_content\n",
    "            if current_section_content is not None: section5_content_length.append(len(current_section_content))\n",
    "        elif section_idx == 6:\n",
    "            current_section_content = package_leaflets[leaflet_idx].section6.section_content\n",
    "            if current_section_content is not None: section6_content_length.append(len(current_section_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1:  962.7945205479452\n",
      "Section 2:  4559.597402597403\n",
      "Section 3:  2300.4912414318355\n",
      "Section 4:  3452.67722007722\n",
      "Section 5:  630.4846416382253\n",
      "Section 6:  981.7040427154843\n"
     ]
    }
   ],
   "source": [
    "print('Section 1: ', np.mean(section1_content_length))\n",
    "print('Section 2: ', np.mean(section2_content_length))\n",
    "print('Section 3: ', np.mean(section3_content_length))\n",
    "print('Section 4: ', np.mean(section4_content_length))\n",
    "print('Section 5: ', np.mean(section5_content_length))\n",
    "print('Section 6: ', np.mean(section6_content_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents ---  1336\n",
      "Num. of section 1 not None:  1314\n",
      "Num. of section 2 not None:  1309\n",
      "Num. of section 3 not None:  1313\n",
      "Num. of section 4 not None:  1295\n",
      "Num. of section 5 not None:  1172\n",
      "Num. of section 6 not None:  1311\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents --- \", len(package_leaflets))\n",
    "\n",
    "print('Num. of section 1 not None: ', len(section1_content_length))\n",
    "print('Num. of section 2 not None: ', len(section2_content_length))\n",
    "print('Num. of section 3 not None: ', len(section3_content_length))\n",
    "print('Num. of section 4 not None: ', len(section4_content_length))\n",
    "print('Num. of section 5 not None: ', len(section5_content_length))\n",
    "print('Num. of section 6 not None: ', len(section6_content_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the entities of each section are sorted by \"BeginOffset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sort_key(entity):\n",
    "    return entity['BeginOffset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_order_entities(section_entities):\n",
    "    \n",
    "    sorted_entities = sorted(section_entities, key=_sort_key)\n",
    "    \n",
    "    if sorted_entities == section_entities: return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for leaflet in package_leaflets:\n",
    "    \n",
    "    current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                leaflet.section3, leaflet.section4, \n",
    "                                leaflet.section5, leaflet.section6]\n",
    "    \n",
    "    for current_section in current_leaflet_sections:\n",
    "        \n",
    "        if current_section.entity_recognition is None:\n",
    "            continue\n",
    "            \n",
    "        assert test_order_entities(current_section.entity_recognition) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of sections with section_content != None and entities != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  [Before splitting] Number of not None sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sections(dataset):\n",
    "    count_sections = {\n",
    "        '1': 0,\n",
    "        '2': 0,\n",
    "        '3': 0,\n",
    "        '4': 0,\n",
    "        '5': 0,\n",
    "        '6': 0\n",
    "    }\n",
    "\n",
    "    for leaflet in dataset:\n",
    "\n",
    "        current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                    leaflet.section3, leaflet.section4, \n",
    "                                    leaflet.section5, leaflet.section6]\n",
    "\n",
    "        for section_index, current_section in enumerate(current_leaflet_sections):\n",
    "\n",
    "            # skip None (duplicates should be skipped)\n",
    "            if current_section.section_content is None or current_section.entity_recognition is None:\n",
    "                continue\n",
    "\n",
    "            # save the detected entities in SectionLeaflet\n",
    "            if (section_index+1) == 1:\n",
    "                count_sections['1'] += 1\n",
    "            elif (section_index+1) == 2:\n",
    "                count_sections['2'] += 1\n",
    "            elif (section_index+1) == 3:\n",
    "                count_sections['3'] += 1\n",
    "            elif (section_index+1) == 4:\n",
    "                count_sections['4'] += 1\n",
    "            elif (section_index+1) == 5:\n",
    "                count_sections['5'] += 1\n",
    "            elif (section_index+1) == 6:\n",
    "                count_sections['6'] += 1\n",
    "    \n",
    "    return count_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1311, '2': 1309, '3': 1310, '4': 1287, '5': 861, '6': 1304}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections(package_leaflets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add <product_name> as first entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for leaflet in package_leaflets:\n",
    "\n",
    "    current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                leaflet.section3, leaflet.section4, \n",
    "                                leaflet.section5, leaflet.section6]\n",
    "\n",
    "    for section_index, current_section in enumerate(current_leaflet_sections):\n",
    "\n",
    "        # skip None\n",
    "        if current_section.entity_recognition is None or len(current_section.entity_recognition) == 0:\n",
    "            current_section.entity_recognition = None\n",
    "            continue\n",
    "        \n",
    "        # extract results of NER\n",
    "        section_entity_recognition = current_section.entity_recognition\n",
    "\n",
    "        # add product_name as 1st Entity\n",
    "        section_entity_recognition.insert(0, {'Text':leaflet.product_name.lower(), 'Type':'PRODUCT_NAME', 'BeginOffset': 0, 'EndOffset': 0})\n",
    "        \n",
    "        # update the info in dataset\n",
    "        current_section.entity_recognition = section_entity_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test adding <product_name> as 1st entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for leaflet_idx in range(len(package_leaflets)):\n",
    "    \n",
    "    for section_idx in range(1,7):\n",
    "        \n",
    "        if section_idx == 1: current_entities = package_leaflets[leaflet_idx].section1.entity_recognition\n",
    "        elif section_idx == 2: current_entities = package_leaflets[leaflet_idx].section2.entity_recognition\n",
    "        elif section_idx == 3: current_entities = package_leaflets[leaflet_idx].section3.entity_recognition\n",
    "        elif section_idx == 4: current_entities = package_leaflets[leaflet_idx].section4.entity_recognition\n",
    "        elif section_idx == 5: current_entities = package_leaflets[leaflet_idx].section5.entity_recognition\n",
    "        elif section_idx == 6: current_entities = package_leaflets[leaflet_idx].section6.entity_recognition\n",
    "        \n",
    "        if current_entities is not None:\n",
    "            assert current_entities[0]['Type'] == 'PRODUCT_NAME'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train-dev-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train-valid-test (0.9-0.1-0.1) and shuffle \n",
    "\n",
    "# train - test\n",
    "train_leaflets, test_leaflets = train_test_split(package_leaflets, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# train - valid \n",
    "train_leaflets, valid_leaflets = train_test_split(train_leaflets, test_size=134, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test distribution of section in train-dev-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1049, '2': 1046, '3': 1044, '4': 1028, '5': 682, '6': 1042}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections(train_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 129, '2': 130, '3': 133, '4': 130, '5': 98, '6': 129}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections(valid_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 133, '2': 133, '3': 133, '4': 129, '5': 81, '6': 133}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sections(test_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(\"datasets/LEAFLET_TRAIN_DATASET.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_leaflets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(\"datasets/LEAFLET_VALID_DATASET.pickle\", \"wb\") as f:\n",
    "    pickle.dump(valid_leaflets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(\"datasets/LEAFLET_TEST_DATASET.pickle\", \"wb\") as f:\n",
    "    pickle.dump(test_leaflets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
