{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TRAIN_DATASET.pickle\", \"rb\") as f:\n",
    "    train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_VALID_DATASET.pickle\", \"rb\") as f:\n",
    "    valid_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load array of objects, where object - class Leaflet\n",
    "with open(\"/home/ruslan_yermakov/nlg-ra/datasets/LEAFLET_TEST_DATASET.pickle\", \"rb\") as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce same output as the script *create_dataset* from data2text-plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "\n",
    "section_content -------- true text   \n",
    "\n",
    "entity_recognition  ------- actually input - set of records   \n",
    "\n",
    "Set of records in order they appear in section_content    -------- content plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECORD_DELIM = \" \"\n",
    "DELIM = u\"ï¿¨\"\n",
    "\n",
    "HOME = \"HOME\"\n",
    "AWAY = \"AWAY\"\n",
    "\n",
    "# ENTITY = \"Indication\"\n",
    "ENTITY = \"Section3\"\n",
    "\n",
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure - sorted entities\n",
    "def _sort_key(entity):\n",
    "    return entity['BeginOffset']\n",
    "\n",
    "def test_order_entities(section_entities):\n",
    "    \n",
    "    sorted_entities = sorted(section_entities, key=_sort_key)\n",
    "    \n",
    "    if sorted_entities == section_entities: return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def test_order_dataset(dataset):\n",
    "    for leaflet in dataset:\n",
    "\n",
    "        current_leaflet_sections = [leaflet.section1, leaflet.section2, \n",
    "                                    leaflet.section3, leaflet.section4, \n",
    "                                    leaflet.section5, leaflet.section6]\n",
    "\n",
    "        for current_section in current_leaflet_sections:\n",
    "\n",
    "            if current_section.entity_recognition is None:\n",
    "                continue\n",
    "\n",
    "            assert test_order_entities(current_section.entity_recognition) == True\n",
    "\n",
    "            \n",
    "test_order_dataset(train_dataset)\n",
    "test_order_dataset(valid_dataset)\n",
    "test_order_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_contentplan(dataset):\n",
    "    \"\"\"\n",
    "    Transform dataset to be a suitable format for model data2text-plan\n",
    "    \"\"\"\n",
    "    \n",
    "    # array to store section3 of each leaflet\n",
    "    summaries_leaflets = []\n",
    "    \n",
    "    # array to store content plan of each leaflet\n",
    "    content_plan_leaflets = []\n",
    "    \n",
    "    for leaflet in dataset:\n",
    "        \n",
    "        # extract section3 content\n",
    "        section3_content = leaflet.section3.section_content\n",
    "        # extract results of NER\n",
    "        section3_entity_recognition = leaflet.section3.entity_recognition\n",
    "        \n",
    "        # skip if either input or output is None\n",
    "        if section3_content is None or section3_entity_recognition is None:\n",
    "            continue\n",
    "        \n",
    "        # get the content plan of each section\n",
    "        content_plan_section3 = ''\n",
    "\n",
    "        for entity in section3_entity_recognition:\n",
    "            entity_value = entity['Text'] if len(entity['Text'].split(\" \")) == 0 else (\"_\").join(entity['Text'].split(\" \"))\n",
    "            entity_type = entity['Type'] if entity['Type'] is not None and len(entity['Type']) > 0 else entity['Category']\n",
    "            \n",
    "            # randomly choose HOME or AWAY\n",
    "            if random.randint(1,2) == 1:\n",
    "                content_plan_section3 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + HOME\n",
    "            else:\n",
    "                content_plan_section3 += entity_value + DELIM + ENTITY + DELIM + entity_type + DELIM + AWAY\n",
    "            \n",
    "            if section3_entity_recognition.index(entity) != len(section3_entity_recognition) - 1:\n",
    "                content_plan_section3 += \" \"\n",
    "            else:\n",
    "                content_plan_section3 += \" \" + \"\\n\"\n",
    "\n",
    "\n",
    "        content_plan_leaflets.append(content_plan_section3)\n",
    "\n",
    "        # get the section3 content\n",
    "        # make sure to have punctuations as a separate token\n",
    "        section3_content = wordpunct_tokenize(section3_content)\n",
    "        \n",
    "        # back to string\n",
    "        section3_content = \" \".join(section3_content)\n",
    "        \n",
    "        # add \"\\n\" at the end\n",
    "        section3_content = section3_content + \"\\n\"\n",
    "        \n",
    "        summaries_leaflets.append(section3_content)\n",
    "    \n",
    "    return (content_plan_leaflets, summaries_leaflets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_records(records):\n",
    "    \"\"\"\n",
    "    To src_train.txt and src_valid.txt pre-append these special characters, according to data2text-plan project\n",
    "    \"\"\"\n",
    "    \n",
    "    record = []\n",
    "    record.append(UNK_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(BOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    record = []\n",
    "    record.append(EOS_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    record.append(PAD_WORD)\n",
    "    records.append(DELIM.join(record))\n",
    "    \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the src_train - training data to be input to the model\n",
    "\n",
    "def create_src_table(content_plan_leaflets):\n",
    "    \"\"\"\n",
    "    Create src_train - input \"table\" to the model\n",
    "    \n",
    "    Idea: - we do not have a table, so randomized the records in content plan\n",
    "    \n",
    "    Update: - do not randomize - make it easier for the model to learn\n",
    "    \"\"\"\n",
    "    \n",
    "    # store input \"table\" of each leaflet in array\n",
    "    src_leaflets = []\n",
    "    \n",
    "    for leaflet_content_plan in content_plan_leaflets:\n",
    "        # remove the end symbol ('\\n') of the string\n",
    "        leaflet_content_plan = leaflet_content_plan[:-2]\n",
    "\n",
    "        # split string into a list of records\n",
    "        leaflet_content_plan_collection = leaflet_content_plan.split(\" \")\n",
    "        \n",
    "        # do not do\n",
    "        # randomly shuffle records in a list\n",
    "        # random.shuffle(leaflet_content_plan_collection)\n",
    "        \n",
    "        # add special symbols to the begining\n",
    "        special_symbols = []\n",
    "        special_symbols = add_special_records(special_symbols)\n",
    "        \n",
    "        # create a string containing all the records and special_symbols in the begining\n",
    "        src_leaflet_section1 = ''\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(special_symbols)\n",
    "        src_leaflet_section1 += \" \"\n",
    "\n",
    "        src_leaflet_section1 += \" \".join(leaflet_content_plan_collection)\n",
    "        src_leaflet_section1 += '\\n'\n",
    "\n",
    "\n",
    "        src_leaflets.append(src_leaflet_section1)\n",
    "    \n",
    "    return src_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE = 'src_train.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT = 'train_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_train, leaflets_inter_contentplan_train, leaflets_tgt_train = create_training_sets(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    src_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tgt_train.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/train_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    inter_train_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================\n",
    "## Validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_sets(dataset):\n",
    "    \n",
    "    # Output files    \n",
    "    INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage\n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to corresponding files\n",
    "    output_file = open(INTER_CONTENT_PLAN_VALID, 'w')\n",
    "    for content_plan in content_plan_leaflets:\n",
    "        output_file.write(content_plan)\n",
    "    output_file.close()\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_VALID, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_VALID, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "    \n",
    "    ### create last file needed - e.g (rotowire/train_content_plan.txt)\n",
    "    inputs = []\n",
    "    content_plans = []\n",
    "    with codecs.open(INTER_CONTENT_PLAN_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            content_plans.append(line.split())\n",
    "\n",
    "    with codecs.open(SRC_FILE_VALID, \"r\", \"utf-8\") as corpus_file:\n",
    "        for i, line in enumerate(corpus_file):\n",
    "            inputs.append(line.split())\n",
    "    \n",
    "    # basically - now content plan POINTs to index in the training dataset\n",
    "    # content_plan - collection of indexes where each index points to record in training dataset - training_dataset[index]\n",
    "    \n",
    "    outputs = []\n",
    "\n",
    "    for i, training_sample in enumerate(inputs):\n",
    "        content_plan = content_plans[i]\n",
    "        output = []\n",
    "        for record in content_plan:\n",
    "            output.append(str(training_sample.index(record)))\n",
    "        outputs.append(\" \".join(output))\n",
    "        \n",
    "    # write to a file\n",
    "\n",
    "    output_file = open(CONTENT_PLAN_OUT_VALID, 'w')\n",
    "\n",
    "    # add \\n to the end of the string\n",
    "    output_file.write(\"\\n\".join(outputs))\n",
    "    # add \\n between content plans\n",
    "    output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    return src_leaflets, content_plan_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_valid, leaflets_inter_contentplan_valid, leaflets_tgt_valid = create_validation_sets(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inter/valid_content_plan.txt') as reader:\n",
    "    # This reads the remaining lines from the file object and returns them as a list.\n",
    "    contentplan_valid = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTER_CONTENT_PLAN_VALID = 'inter/valid_content_plan.txt'  # intermediate content plan input to second stage\n",
    "    SRC_FILE_VALID = 'src_valid.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_VALID = \"tgt_valid.txt\"  # tgt file of second stage\n",
    "    CONTENT_PLAN_OUT_VALID = 'valid_content_plan.txt'  # content plan output of first stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================\n",
    "## Test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sets(dataset):\n",
    "    \n",
    "    # Output files\n",
    "    SRC_FILE_TEST = 'test/src_test.txt'  # src file input to first stage\n",
    "    TRAIN_TGT_FILE_TEST = \"test/tgt_test.txt\"  # tgt file of second stage \n",
    "    \n",
    "    \n",
    "    # Create src - content_plan - summary\n",
    "    content_plan_leaflets, summaries_leaflets = create_summary_contentplan(dataset)\n",
    "    src_leaflets = create_src_table(content_plan_leaflets)\n",
    "    \n",
    "    # save to just summary and src data\n",
    "    \n",
    "    summary_file = open(TRAIN_TGT_FILE_TEST, 'w')\n",
    "    for summary_leaflet in summaries_leaflets:\n",
    "        summary_file.write(summary_leaflet)\n",
    "    summary_file.close()\n",
    "    \n",
    "    src_file = open(SRC_FILE_TEST, 'w')\n",
    "    for src_instance in src_leaflets:\n",
    "        src_file.write(src_instance)\n",
    "    src_file.close()\n",
    "\n",
    "    return src_leaflets, summaries_leaflets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaflets_src_test, leaflets_tgt_test = create_test_sets(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================================================================\n",
    "## Creating *train-roto-ptrs.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files\n",
    "INTER_CONTENT_PLAN = 'inter/train_content_plan.txt'  # intermediate content plan input to second stage\n",
    "TRAIN_TGT_FILE = \"tgt_train.txt\"  # tgt file of second stage\n",
    "OUTPUT_FILE = \"train-roto-ptrs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_TGT_FILE) as reader:\n",
    "    leaflet_tgt_train = reader.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INTER_CONTENT_PLAN) as reader:\n",
    "    leaflets_inter_content_plan = reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eg: the last entry 245,39 in train_roto_ptrs[1] indicates that the 245th token in summary matches with 39th content plan entry.  \n",
    "\n",
    "Phoenix ----> Phoenixï¿¨Sunsï¿¨TEAM-CITYï¿¨HOME  \n",
    "Suns ----> Sunsï¿¨Sunsï¿¨TEAM-NAMEï¿¨HOME  \n",
    "39 ----> 39ï¿¨Sunsï¿¨TEAM-WINSï¿¨HOME  \n",
    "38 ----> 38ï¿¨Sunsï¿¨TEAM-LOSSESï¿¨HOME  \n",
    "87 ----> 87ï¿¨Sunsï¿¨TEAM-PTSï¿¨HOME  \n",
    "85 ----> 85ï¿¨Jazzï¿¨TEAM-PTSï¿¨AWAY  \n",
    "Utah ----> Utahï¿¨Jazzï¿¨TEAM-CITYï¿¨AWAY  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"always use cystagon exactly as your doctor or your child ' s doctor has told you . you should check with your doctor if you are not sure . the dose of cystagon prescribed for you or your child will depend on your or your child ' s age and weight . for children up to age 12 years , the dose will be based on the body size ( surface area ), the usual dose being 1 . 30 g / m2 of body surface area per day . for patients over age 12 and over 50 kg weight , the usual dose is 2g / day . in any case the usual dose should not exceed 1 . 95 g / m2 / day . cystagon should be taken or given only by mouth and exactly as your or your child ' s doctor directs . in order for cystagon to work correctly , you must do the following : - follow your doctor ' s directions exactly . do not increase or decrease the amount of medicine without your doctor ' s approval . - hard capsules should not be given to children under approximately six years of age because they may not be able to swallow them and they may choke . for children under approximately six years of age , the hard capsule may be opened and the contents sprinkled on food ( e . g . milk , potatoes or starch based foods ) or mixed in formula . do not add to acidic drinks e . g . orange juice . consult the doctor for complete directions . - your or your child ' s medical treatment may include , in addition to cystagon , one or more supplements to replace important electrolytes lost through the kidneys . it is important to take or give these supplements exactly as instructed . if several doses of the supplements are missed or weakness or drowsiness develops , call the doctor for instructions . - regular blood tests to measure the amount of cystine inside white blood cells are necessary to help determine the correct dose of cystagon . your or your child ' s doctor will arrange for the blood tests to be done . regular blood and urine tests to measure the levels of the body ' s important electrolytes are also necessary to help your or your child ' s doctor correctly adjust the doses of these supplements . cystagon should be taken 4 times a day , every 6 hours , preferably just after or with food . it is important to take the dose as close to every 6 hours as possible . treatment with cystagon should continue indefinitely , as instructed by your doctor . if you use more cystagon than you should : you should contact your or your child ' s doctor or the hospital emergency department immediately if more medicine has been taken than has been prescribed , drowsiness develops . if you forget to take cystagon : if a dose of medicine is missed , it should be taken as soon as possible . however if it is within two hours of the next dose , skip the missed dose and go back to the regular dosing schedule . do not take a double dose to make up for a forgotten dose .\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cystagonï¿¨Section3ï¿¨PRODUCT_NAMEï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨AWAY 12ï¿¨Section3ï¿¨AGEï¿¨AWAY 1.30ï¿¨Section3ï¿¨NUMBERï¿¨AWAY 12ï¿¨Section3ï¿¨NUMBERï¿¨HOME 50ï¿¨Section3ï¿¨NUMBERï¿¨AWAY 1.95ï¿¨Section3ï¿¨NUMBERï¿¨HOME cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME mouthï¿¨Section3ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME hard_capsulesï¿¨Section3ï¿¨TREATMENTï¿¨AWAY starch_based_foodsï¿¨Section3ï¿¨TREATMENTï¿¨AWAY your_child's_medical_treatmentï¿¨Section3ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME important_electrolytesï¿¨Section3ï¿¨TESTï¿¨AWAY kidneysï¿¨Section3ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY these_supplementsï¿¨Section3ï¿¨TREATMENTï¿¨HOME the_supplementsï¿¨Section3ï¿¨TREATMENTï¿¨HOME weaknessï¿¨Section3ï¿¨DX_NAMEï¿¨HOME drowsinessï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY regular_blood_testsï¿¨Section3ï¿¨TESTï¿¨AWAY cystine_inside_white_blood_cellsï¿¨Section3ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME the_blood_testsï¿¨Section3ï¿¨TESTï¿¨HOME regular_blood_and_urine_testsï¿¨Section3ï¿¨TESTï¿¨HOME these_supplementsï¿¨Section3ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section3ï¿¨TREATMENT_NAMEï¿¨HOME 4ï¿¨Section3ï¿¨NUMBERï¿¨HOME 6ï¿¨Section3ï¿¨NUMBERï¿¨AWAY 6ï¿¨Section3ï¿¨NUMBERï¿¨HOME treatmentï¿¨Section3ï¿¨TREATMENTï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨AWAY emergency_departmentï¿¨Section3ï¿¨ADDRESSï¿¨HOME drowsinessï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY cystagonï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME medicineï¿¨Section3ï¿¨TREATMENTï¿¨AWAY the_regular_dosing_scheduleï¿¨Section3ï¿¨TREATMENTï¿¨AWAY a_double_doseï¿¨Section3ï¿¨TREATMENTï¿¨AWAY \\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "roto_pts_content = []\n",
    "\n",
    "# for each leaflet\n",
    "for leflet_num in range(len(leaflet_tgt_train)):\n",
    "    \n",
    "    # get current leaflet and content plan\n",
    "    current_leaflet = leaflet_tgt_train[leflet_num].split()\n",
    "    current_content_plan = leaflets_inter_content_plan[leflet_num].split()\n",
    "    \n",
    "    # get the values of content plan\n",
    "    instances = []\n",
    "    for entry in current_content_plan:\n",
    "        record_values = entry.split(DELIM)[0]\n",
    "        instances.append(record_values)\n",
    "    \n",
    "    # pairs (index_tgt, index_contentplan) for each leaflet\n",
    "    current_str = []\n",
    "    \n",
    "    # for each token in current summary\n",
    "    for token_pos in range(len(current_leaflet)):\n",
    "        \n",
    "        # get token\n",
    "        token = current_leaflet[token_pos]\n",
    "        \n",
    "        # possible tokens if 2 words in content plan like 'immunodeficiency_syndrome'\n",
    "        if token_pos < (len(current_leaflet)-1):\n",
    "            token_2words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1]\n",
    "        else:\n",
    "            token_2words = 'something that would never be in the section content'\n",
    "        \n",
    "        ### my-new-change\n",
    "        # possible tokens if 3 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-2):\n",
    "            token_3words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2]\n",
    "        else:\n",
    "            token_3words = 'something that would never be in the section content'\n",
    "        \n",
    "        # possible tokens if 4 words in content plan\n",
    "        if token_pos < (len(current_leaflet)-3):\n",
    "            token_4words = current_leaflet[token_pos] + \"_\" + current_leaflet[token_pos+1] + \"_\" + current_leaflet[token_pos+2] + \"_\" + current_leaflet[token_pos+3]\n",
    "        else:\n",
    "            token_4words = 'something that would never be in the section content'\n",
    "        \n",
    "        \n",
    "        for content_plan_index in range(len(instances)):\n",
    "                \n",
    "            if token_4words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_3words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token_2words == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "            \n",
    "            if token == instances[content_plan_index]:\n",
    "                # mask the corresponding position in content plan\n",
    "                instances[content_plan_index] = \"MASKED\"\n",
    "                pair = str(token_pos) + \",\" + str(content_plan_index)\n",
    "                current_str.append(pair)\n",
    "                # find just one match\n",
    "                break\n",
    "    \n",
    "    # join pairs into string with \" \" between pairs\n",
    "    current_str = \" \".join(current_str)\n",
    "    \n",
    "    # add \"\\n\" at the end\n",
    "    current_str += \"\\n\"\n",
    "    \n",
    "    roto_pts_content.append(current_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"train-roto-ptrs.txt\"\n",
    "\n",
    "src_file = open(OUTPUT_FILE, 'w')\n",
    "for src_instance in roto_pts_content:\n",
    "    src_file.write(src_instance)\n",
    "src_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,1 48,0 54,3 75,2 87,5 99,6 114,4 128,8 131,9 135,10 138,11 141,12 143,13 145,14 146,15 149,16 159,17 163,18 181,20 183,21 185,22 190,23 198,24 201,25 219,27 228,7 277,29 297,30\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roto_pts_content[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(roto_pts_content[index].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaflets_inter_content_plan[index].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'medicine', 'exactly', 'as'] ----> this_medicineï¿¨Section3ï¿¨TREATMENTï¿¨HOME\n",
      "['kinzalkomb', 'with', 'or', 'without'] ----> kinzalkombï¿¨Section3ï¿¨PRODUCT_NAMEï¿¨AWAY\n",
      "['the', 'tablets', 'should', 'be'] ----> the_tabletsï¿¨Section3ï¿¨TREATMENTï¿¨AWAY\n",
      "['kinzalkomb', 'every', 'day', 'until'] ----> kinzalkombï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['liver', 'is', 'not', 'working'] ----> liverï¿¨Section3ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME\n",
      "['40', 'mg', '/', '12'] ----> 40ï¿¨Section3ï¿¨NUMBERï¿¨HOME\n",
      "['kinzalkomb', 'than', 'you', 'should'] ----> kinzalkombï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['symptoms', 'such', 'as', 'low'] ----> symptomsï¿¨Section3ï¿¨PROBLEMï¿¨HOME\n",
      "['low', 'blood', 'pressure', 'and'] ----> low_blood_pressureï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['rapid', 'heartbeat', '.', 'slow'] ----> rapid_heartbeatï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['slow', 'heartbeat', ',', 'dizziness'] ----> slow_heartbeatï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['dizziness', ',', 'vomiting', ','] ----> dizzinessï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['vomiting', ',', 'reduced', 'kidney'] ----> vomitingï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['reduced', 'kidney', 'function', 'including'] ----> reduced_kidney_functionï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['kidney', 'function', 'including', 'kidney'] ----> kidneyï¿¨Section3ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY\n",
      "['kidney', 'failure', ',', 'have'] ----> kidney_failureï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['the', 'hydrochlorothiazide', 'component', ','] ----> the_hydrochlorothiazide_componentï¿¨Section3ï¿¨PROBLEMï¿¨HOME\n",
      "['markedly', 'low', 'blood', 'pressure'] ----> markedly_low_blood_pressureï¿¨Section3ï¿¨PROBLEMï¿¨HOME\n",
      "['nausea', ',', 'sleepiness', 'and'] ----> nauseaï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['sleepiness', 'and', 'muscle', 'cramps'] ----> sleepinessï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['muscle', 'cramps', 'and', '/'] ----> muscle_crampsï¿¨Section3ï¿¨DX_NAMEï¿¨HOME\n",
      "['irregular', 'heartbeat', 'associated', 'with'] ----> irregular_heartbeatï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "['drugs', 'such', 'as', 'digitalis'] ----> drugsï¿¨Section3ï¿¨TREATMENTï¿¨AWAY\n",
      "['digitalis', 'or', 'certain', 'anti'] ----> digitalisï¿¨Section3ï¿¨GENERIC_NAMEï¿¨AWAY\n",
      "['emergency', 'department', 'immediately', '.'] ----> emergency_departmentï¿¨Section3ï¿¨ADDRESSï¿¨HOME\n",
      "['kinzalkomb', 'if', 'you', 'forget'] ----> kinzalkombï¿¨Section3ï¿¨GENERIC_NAMEï¿¨HOME\n",
      "['a', 'double', 'dose', 'to'] ----> a_double_doseï¿¨Section3ï¿¨TREATMENTï¿¨HOME\n",
      "['this', 'medicine', ',', 'ask'] ----> this_medicineï¿¨Section3ï¿¨TREATMENTï¿¨AWAY\n"
     ]
    }
   ],
   "source": [
    "content_plan_indeces = []\n",
    "\n",
    "for pair in roto_pts_content[index].split():\n",
    "    pair = pair.split(\",\")\n",
    "    \n",
    "    a = int(pair[0])\n",
    "    b = int(pair[1])\n",
    "    \n",
    "    content_plan_indeces.append(b)\n",
    "    \n",
    "    print(leaflet_tgt_train[index].split()[a:a+4], end=\" ----> \")\n",
    "    print(leaflets_inter_content_plan[index].split()[b])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_blood_levels_of_potassiumï¿¨Section3ï¿¨DX_NAMEï¿¨AWAY\n",
      "certain_anti-arrhythmic_treatmentsï¿¨Section3ï¿¨TREATMENTï¿¨HOME\n",
      "kinzalkombï¿¨Section3ï¿¨GENERIC_NAMEï¿¨AWAY\n"
     ]
    }
   ],
   "source": [
    "# check out pairs missed\n",
    "for i in range(0, len(leaflets_inter_content_plan[index].split()), 1):\n",
    "    if i not in content_plan_indeces:\n",
    "        print(leaflets_inter_content_plan[index].split()[i])\n",
    "        \n",
    "# explanation - 3-word long token\n",
    "# explanation - hiv_infection ---> bc NER outputs - 'hiv', 'hiv_infection' - in content plan I have 2 tokens starting with 'hiv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"zyclara 3 . 75 % cream contains the active substance imiquimod , which is an immune response modifier ( to stimulate the human immune system ). this medicine is prescribed for the treatment of actinic keratosis in adults . this medicine stimulates your body ' s own immune system to produce natural substances which help fight your actinic keratosis . actinic keratosis appears as rough areas of skin found in people who have been exposed to a lot of sunshine over the course of their lifetime . these areas can be the same colour as your skin or are greyish , pink , red or brown . they can be flat and scaly , or raised , rough , hard and warty . this medicine should only be used for actinic keratosis on the face or scalp if your doctor has decided that it is the most appropriate treatment for you .\\n\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflet_tgt_train[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zyclaraï¿¨Section1ï¿¨PRODUCT_NAMEï¿¨HOME zyclaraï¿¨Section1ï¿¨BRAND_NAMEï¿¨HOME 3.75ï¿¨Section1ï¿¨NUMBERï¿¨HOME the_active_substance_imiquimodï¿¨Section1ï¿¨TREATMENTï¿¨AWAY an_immune_response_modifierï¿¨Section1ï¿¨TREATMENTï¿¨AWAY this_medicineï¿¨Section1ï¿¨TREATMENTï¿¨AWAY actinic_keratosisï¿¨Section1ï¿¨DX_NAMEï¿¨HOME this_medicineï¿¨Section1ï¿¨TREATMENTï¿¨AWAY natural_substancesï¿¨Section1ï¿¨TREATMENTï¿¨AWAY your_actinic_keratosisï¿¨Section1ï¿¨PROBLEMï¿¨HOME actinic_keratosisï¿¨Section1ï¿¨DX_NAMEï¿¨HOME skinï¿¨Section1ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME skinï¿¨Section1ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME greyishï¿¨Section1ï¿¨DX_NAMEï¿¨AWAY pinkï¿¨Section1ï¿¨DX_NAMEï¿¨HOME redï¿¨Section1ï¿¨DX_NAMEï¿¨HOME flatï¿¨Section1ï¿¨DX_NAMEï¿¨AWAY scalyï¿¨Section1ï¿¨DX_NAMEï¿¨HOME wartyï¿¨Section1ï¿¨DX_NAMEï¿¨HOME this_medicineï¿¨Section1ï¿¨TREATMENTï¿¨AWAY actinic_keratosisï¿¨Section1ï¿¨DX_NAMEï¿¨AWAY faceï¿¨Section1ï¿¨SYSTEM_ORGAN_SITEï¿¨AWAY scalpï¿¨Section1ï¿¨SYSTEM_ORGAN_SITEï¿¨HOME \\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaflets_inter_content_plan[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
